{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Contents\">Contents<a href=\"#Contents\"></a></h2>\n",
    "        <ol>\n",
    "        <li><a class=\"\" href=\"#Imports\">Imports</a></li>\n",
    "<li><a class=\"\" href=\"#Course-2\">Course 2</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Improved-Implementation-of-softmax\">Improved Implementation of softmax</a></li>\n",
    "<li><a class=\"\" href=\"#Measuring-Purity\">Measuring Purity</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Entropy\">Entropy</a></li>\n",
    "<li><a class=\"\" href=\"#Gini-Index\">Gini Index</a></li>\n",
    "</ol><li><a class=\"\" href=\"#Choosing-a-split:-Information-Gain\">Choosing a split: Information Gain</a></li>\n",
    "</ol><li><a class=\"\" href=\"#Course-3\">Course 3</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Anomaly-detection-vs.-supervised-learning\">Anomaly detection vs. supervised learning</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Anomaly-detection\">Anomaly detection</a></li>\n",
    "<li><a class=\"\" href=\"#Supervised-learning\">Supervised learning</a></li>\n",
    "</ol><li><a class=\"\" href=\"#Recommender-Systems\">Recommender Systems</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Deep-Learning-for-recommender-systems\">Deep Learning for recommender systems</a></li>\n",
    "</ol><li><a class=\"\" href=\"#Reinforcement-Learning\">Reinforcement Learning</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Markov-Decision-Process\">Markov Decision Process</a></li>\n",
    "<li><a class=\"\" href=\"#The-Mars-Rover-Example\">The Mars Rover Example</a></li>\n",
    "<li><a class=\"\" href=\"#State\">State</a></li>\n",
    "<li><a class=\"\" href=\"#Action\">Action</a></li>\n",
    "<li><a class=\"\" href=\"#Reward\">Reward</a></li>\n",
    "<li><a class=\"\" href=\"#Return\">Return</a></li>\n",
    "<li><a class=\"\" href=\"#Policy\">Policy</a></li>\n",
    "<li><a class=\"\" href=\"#Review\">Review</a></li>\n",
    "<li><a class=\"\" href=\"#State-Action-Value-Function:-Q\">State-Action Value Function: Q</a></li>\n",
    "<li><a class=\"\" href=\"#Bellman-Equation\">Bellman Equation</a></li>\n",
    "<li><a class=\"\" href=\"#Continuous-State\">Continuous State</a></li>\n",
    "<li><a class=\"\" href=\"#Lunar-Lander\">Lunar Lander</a></li>\n",
    "<li><a class=\"\" href=\"#Training-a-Neural-Network\">Training a Neural Network</a></li>\n",
    "<li><a class=\"\" href=\"#$%5Cepsilon$---Greedy-Policy\">$\\epsilon$ - Greedy Policy</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved Implementation of softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000200000000000000\n"
     ]
    }
   ],
   "source": [
    "res = 2/10000\n",
    "print(f\"{res:.18f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000199999999999978\n"
     ]
    }
   ],
   "source": [
    "res = 1+ 1/10000 - (1-1/10000)\n",
    "print(f\"{res:.18f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This happens because computer have limited precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for softmax, instead of using:\n",
    "$$\n",
    "a = g(z) = \\frac{1}{1-e^{-z}}\\\\\n",
    "\\mathcal{L} = -\\sum_{i=1}^n y_i \\log a_i - (1-y_i) \\log (1-a_i)\n",
    "$$\n",
    "We can put the $z$ into the log:\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{i=1}^n y_i \\log \\frac{1}{1-e^{-z_i}} - (1-y_i) \\log (1-\\frac{1}{1-e^{-z_i}})\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives Tensorflow flexibility to rearrange terms so it can calculate the loss more efficiently. The way we do this in Tensorflow is by using the `linear` as the last layer activation and then calculating the `BinaryCrossentropy` loss for logits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(25, input_shape=(1,), activation='relu'),\n",
    "    tf.keras.layers.Dense(15, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Logits is nothing but $\\mathcal{L} = -\\sum_{i=1}^n y_i \\log a_i - (1-y_i) \\log (1-a_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one of the z's really small than e to negative small number becomes very, very small or if one of the z's is a very large number, then e to the z can become a very large number and by rearranging terms, TensorFlow can avoid some of these very small or very large numbers and therefore come up with more actress computation for the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(25, input_shape=(1,), activation='relu'),\n",
    "    tf.keras.layers.Dense(15, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to use `tf.nn.sigmoid` to get the probability of the output as the last layer is now linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Purity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">If the examples are all cats of a single class then that's very pure, if it's all not cats that's also very pure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll have a look at two metrics to measure the purity of a node:\n",
    "1. Entropy\n",
    "2. Gini Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy is a measure of impurity of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Images/0101.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy is defined as a function of $p_1$ which is the fraction of examples which contains the 1 class. The function is:\n",
    "$$\n",
    "H(p_1) = -p_1 \\log_2 p_1 - (1-p_1) \\log_2 (1-p_1)\\\\\n",
    "H(p_1) = -p_1 \\log_2 p_1 - p_0 \\log_2 p_0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $p_0 = 0$ or $p_1=0$, the function should give zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gini Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gini Impurity is a measurement used to build Decision Trees to determine how the features of a dataset should split nodes to form the tree. More precisely, the Gini Impurity of a dataset is a number between 0-0.5, which indicates the likelihood of new, random data being misclassified if it were given a random class label according to the class distribution in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, the Gini Impurity of a dataset is defined as:\n",
    "$$\n",
    "G = 1 - \\sum_{i=1}^n p_i^2\n",
    "$$\n",
    "\n",
    "Where $p_i$ is the probability of a random element in the dataset belonging to class $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://storage.googleapis.com/lds-media/images/gini-impurity-diagram.width-1200.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a split: Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building a decision tree, the way we'll decide what feature to split on at a node will be based on what choice of feature reduces entropy the most. Reduces entropy or reduces impurity, or maximizes purity. In decision tree learning, the reduction of entropy is called information gain. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Images/0102.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the feature which results in the highest resduction in entropy while splitting. In above example, using the Ear shape results in the highest reduction in entropy. Concretely, the information gain is:\n",
    "$$\n",
    "IG = H(p_1^\\text{root}) - \\left(\\frac{N_\\text{left}}{N_\\text{root}} H(p_1^\\text{left}) + \\frac{N_\\text{right}}{N_\\text{root}} H(p_1^\\text{right})\\right)\n",
    "$$\n",
    "where $N_\\text{root}$ is the number of examples in the root node, $N_\\text{left}$ is the number of examples in the left node, and $N_\\text{right}$ is the number of examples in the right node. $p_1^\\text{left}$ is the fraction of examples in the left node which are of class 1. $p_1^\\text{right}$ is the fraction of examples in the right node which are of class 1. While $p_1^\\text{root}$ is the fraction of examples in the root node which are of class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection vs. supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use when very small number of positive examples and a large number of negative examples.\n",
    "2. When there are many different types of anamolies. In this case, it's hard to get enough positive examples to train a classifier. Because you might get a different kind of anamoly in the test set in the future. This works because an anamoly detection model will easily detect anamolies that it has never seen before. That is, it tries to learn what is normal and then anything that is not normal is an anamoly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. When there are enough positive examples to train a classifier.\n",
    "2. Supervised learning works by getting a sense of what positive examples look like and it then tries to classify future positive example. So, when there are not enough positive examples, it's hard to get a sense of what positive examples look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For example, instead of try to learn every type of financial fraud (which is hard), we can try to learn what is normal and then anything that is not normal is an anamoly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For span email, there is a pattern, like they try to send us to a website or to click a link. In this case supervised learning will be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Images/0103.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ratings by users, we can determine features of the movies and then use those features to predict the rating of a movie that a user hasn't seen yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Collaborative filtering is a technique that can filter out items that a user might like on the basis of reactions by similar users.\n",
    ">\n",
    "> Content-based filtering is a technique that can filter out items that a user might like on the basis of a description of the item and a profile of the user's preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning for recommender systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use NN's to create a feature vector for users and movies. Then take a dot product which gives a sense of whether the movie will be liked by the user or not.\n",
    "\n",
    "The feature vectors of the movies can be used to find other similar movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement Learning is a type of Machine Learning. It allows machines and software agents to automatically determine the ideal behavior within a specific context, in order to maximize its performance. Simple reward feedback is required for the agent to learn its behavior; this is known as the reinforcement signal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Markov Decision Process (MDP) model contains: \n",
    "\n",
    "* A set of possible world states S.\n",
    "* A set of possible actions A.\n",
    "* A real-valued reward function $R(s,a)$.\n",
    "* A policy the solution of Markov Decision Process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term Markov in the MDP or Markov decision process refers to that the future only depends on the current state and not on anything that might have occurred prior to getting to the current state. In other words, in a Markov decision process, the future depends only on where you are now, not on how you got here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Images/0107.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Mars Rover Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Images/0104.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A State is a set of tokens that represent every state that the agent can be in. In the Mars Rover example, the state is the position of the rover (six of them)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An Action A is a set of all possible actions. $A(s)$ defines the set of actions that can be taken being in state S. In our example, the action is the direction in which the rover can move (left or right)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Reward is a real-valued reward function. $R(s)$ indicates the reward for simply being in the state S. It is an incentive for the agent to learn to be in a state that has a high reward. In our example, the reward is 100 at state 1 and 40 at state 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Return} = \\sum_{t=0}^T \\gamma^t R_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, $\\gamma$ is set very near to 1.0. The return is the sum of the rewards from the current time step to the end of the episode. The discount factor $\\gamma$ is used to reduce the importance of future rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Images/0105.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy $\\pi(s)$ tells what action to take in a given state to maximize the return. It is a mapping from states to actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Images/0106.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State-Action Value Function: Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Images/0108.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the best possible return from state $s$ is $ \\max_{a} Q(s,a)$ while the best action to take in state $s$ is $\\arg\\max_a Q(s,a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bellman equation is a recursive equation that relates the value of a state to the values of the successor states. Mathematically, it is defined as:\n",
    "$$\n",
    "Q(s,a) = R(s,a) + \\gamma \\max_{a'} Q(s',a')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $R(s,a)$ is the reward for taking action $a$ in state $s$. $\\gamma$ is the discount factor. $s'$ is the next state. $a'$ is the next action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Images/0109.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first term is the reward you get right away. The second term is the return if you behave optimally from the next state. The discount factor $\\gamma$ is used to reduce the importance of future rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If s is terminal, then $Q(s,a) = R(s,a)$. The second term is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In case of random actions, the value of the state-action pair is the average of the rewards. The Bellman equation takes form of:\n",
    "> $$Q(s,a) = R(s,a) + \\gamma E[\\max_{a'}Q(s',a')]$$\n",
    "> $E$ is the expectation operator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For real life problems, the state space is continuous and may consist more than one variable. For example, the state of a car may be defined by its position and velocity as well as the angle of the steering wheel. So, the state is\n",
    "$$\n",
    "s = \n",
    "\\begin{bmatrix}\n",
    "x\\\\\n",
    "y\\\\\n",
    "\\theta\\\\\n",
    "\\dot{x}\\\\\n",
    "\\dot{y}\\\\\n",
    "\\dot{\\theta}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the state of a helicopter may be defined by its position and velocity as well as the angle of the rotor blades. So, the state is\n",
    "$$\n",
    "s =\n",
    "\\begin{bmatrix}\n",
    "x\\\\\n",
    "y\\\\\n",
    "z\\\\\n",
    "\\phi\\\\\n",
    "\\theta\\\\\n",
    "\\psi\\\\\n",
    "\\dot{x}\\\\\n",
    "\\dot{y}\\\\\n",
    "\\dot{z}\\\\\n",
    "\\dot{\\phi}\\\\\n",
    "\\dot{\\theta}\\\\\n",
    "\\dot{\\psi}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lunar Lander"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state is:\n",
    "$$\n",
    "s =\n",
    "\\begin{bmatrix}\n",
    "x\\\\\n",
    "y\\\\\n",
    "\\theta\\\\\n",
    "\\dot{x}\\\\\n",
    "\\dot{y}\\\\\n",
    "\\dot{\\theta}\\\\\n",
    "l\\\\\n",
    "r\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $l$ and $r$ are the left and right leg contact with the ground. $l$ and $r$ are 0 or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Images/0110.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is the learn a policy $\\pi$ that maximizes the return. Deep Q-Learning is used to learn the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Images/0111.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this, we start with taking random action and using the Bellman equation to generate a training set. Then we train a neural network to predict the Q value for a given state and action. Then we use the neural network to generate a training set. Then we train the neural network again. This process is repeated until the neural network converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Images/0112.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$s$ and $a$ are used to calculate $x$ and $R(s)$ and $s$ are used to calculate $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Images/0113.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm is called Deep Q-Learning. Instead of outputing a single action, as shown in the above figure, it is more efficient to train a model which outputs all the possible actions. This way, we have to do the inference just one time.\n",
    "\n",
    "![](Images/0114.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon$ - Greedy Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\epsilon$ - Greedy Policy is used to balance exploration and exploitation. With probability $\\epsilon$, the agent takes a random action. With probability $1-\\epsilon$, the agent takes the action that maximizes the return. This works better than just using a policy where we choose action that maximizes the return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be explained by the fact the the NN initializes its parameter randomly and it is quite possible that these parameters are such that the NN outputs a very low Q value for some actions which will be helpful. In this case, the agent will always choose just a subset action. This is called the **exploration-exploitation dilemma**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exploration versus exploitation trade-off refers to how often do you take actions randomly or take actions that may not be the best in order to learn more, versus trying to maximize your return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Images/0115.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reinforcement learning is more sensitive to the choices of the hyperparameters than supervised learning. Using the wrong hyperparameters can cause the agent to learn in a very long time or nothing at all."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('data-science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2efee1efa502125d01e6b4768ba06d9453d29f3642bfd14ad5d4a769de82e88c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
