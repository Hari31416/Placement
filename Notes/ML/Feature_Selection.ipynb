{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Contents\">Contents<a href=\"#Contents\"></a></h2>\n",
    "        <ol>\n",
    "        <li><a class=\"\" href=\"#Feature-Selection\">Feature Selection</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Removing-features-with-low-variance\">Removing features with low variance</a></li>\n",
    "<li><a class=\"\" href=\"#Univariate-feature-selection\">Univariate feature selection</a></li>\n",
    "<li><a class=\"\" href=\"#Recursive-feature-elimination\">Recursive feature elimination</a></li>\n",
    "<li><a class=\"\" href=\"#Feature-selection-using-SelectFromModel\">Feature selection using SelectFromModel</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Example-1:-SVC-Based\">Example 1: SVC Based</a></li>\n",
    "<li><a class=\"\" href=\"#Example-2:-Tree-Based\">Example 2: Tree Based</a></li>\n",
    "</ol><li><a class=\"\" href=\"#Sequential-Feature-Selection\">Sequential Feature Selection</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Forward-SFS\">Forward SFS</a></li>\n",
    "<li><a class=\"\" href=\"#Backward-SFS\">Backward SFS</a></li>\n",
    "</ol><li><a class=\"\" href=\"#Feature-selection-as-part-of-a-pipeline\">Feature selection as part of a pipeline</a></li>\n",
    "</ol><li><a class=\"\" href=\"#Classes\">Classes</a></li>\n",
    "<ol><li><a class=\"\" href=\"#VarianceThreshold\">VarianceThreshold</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Parameters-of-VarianceThreshold\">Parameters of VarianceThreshold</a></li>\n",
    "<li><a class=\"\" href=\"#Attributes-of-VarianceThreshold\">Attributes of VarianceThreshold</a></li>\n",
    "</ol><li><a class=\"\" href=\"#SelectKBest\">SelectKBest</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Parameters-of-SelectKBest\">Parameters of SelectKBest</a></li>\n",
    "<li><a class=\"\" href=\"#Attributes-of-SelectKBest\">Attributes of SelectKBest</a></li>\n",
    "</ol><li><a class=\"\" href=\"#SelectPercentile\">SelectPercentile</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Parameters-of-SelectPercentile\">Parameters of SelectPercentile</a></li>\n",
    "<li><a class=\"\" href=\"#Attributes-of-SelectPercentile\">Attributes of SelectPercentile</a></li>\n",
    "</ol><li><a class=\"\" href=\"#RFE\">RFE</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Parameters-of-RFE\">Parameters of RFE</a></li>\n",
    "<li><a class=\"\" href=\"#Attributes-of-RFE\">Attributes of RFE</a></li>\n",
    "</ol><li><a class=\"\" href=\"#SelectFromModel\">SelectFromModel</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Parameters-of-SelectFromModel\">Parameters of SelectFromModel</a></li>\n",
    "<li><a class=\"\" href=\"#Attributes-of-SelectFromModel\">Attributes of SelectFromModel</a></li>\n",
    "</ol><li><a class=\"\" href=\"#SequentialFeatureSelector\">SequentialFeatureSelector</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Parameters-of-SequentialFeatureSelector\">Parameters of SequentialFeatureSelector</a></li>\n",
    "<li><a class=\"\" href=\"#Attributes-of-SequentialFeatureSelector\">Attributes of SequentialFeatureSelector</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn has a number of tools for feature selection available in the module `feature_selection`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing features with low variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`VarianceThreshold` is a simple baseline approach to feature selection. It removes all features whose variance doesn’t meet some threshold. By default, it removes all zero-variance features, i.e. features that have the same value in all samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, suppose that we have a dataset with boolean features, and we want to remove all features that are either one or zero (on or off) in more than 80% of the samples. Boolean features are Bernoulli random variables, and the variance of such variables is given by\n",
    "$$\\mathrm{Var}[X] = p(1 - p)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can set a threshold of `0.8*(1-0.8)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [1, 0],\n",
       "       [0, 0],\n",
       "       [1, 1],\n",
       "       [1, 0],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]\n",
    "sel = VarianceThreshold(threshold=(.8 * (1 - .8)))\n",
    "sel.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Univariate feature selection works by selecting the best features based on univariate statistical tests. It can be seen as a preprocessing step to an estimator. Scikit-learn exposes feature selection routines as objects that implement the `transform` method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **`SelectKBest`** removes all but the  highest scoring features\n",
    "\n",
    "* **`SelectPercentile`** removes all but a user-specified highest scoring percentage of features\n",
    "\n",
    "* using common univariate statistical tests for each feature: false positive rate SelectFpr, false discovery rate SelectFdr, or family wise error SelectFwe.\n",
    "\n",
    "* **`GenericUnivariateSelect`** allows to perform univariate feature selection with a configurable strategy. This allows to select the best univariate selection strategy with hyper-parameter search estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(150, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "X, y = load_iris(return_X_y=True)\n",
    "print(X.shape)\n",
    "\n",
    "X_new = SelectKBest(chi2, k=2).fit_transform(X, y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These objects take as input a scoring function that returns univariate scores and p-values (or only scores for `SelectKBest` and `SelectPercentile`):\n",
    "\n",
    "* For regression: *f_regression*, *mutual_info_regression*\n",
    "\n",
    "* For classification: *chi2*, *f_classif*, *mutual_info_classif*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive feature elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (`RFE`) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through any specific attribute (such as `coef_`, `feature_importances_`) or callable. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n",
    "\n",
    "`RFECV` performs `RFE` in a cross-validation loop to find the optimal number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "[False False  True  True]\n",
      "[3 2 1 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(150, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "X, y = load_iris(return_X_y=True)\n",
    "print(X.shape)\n",
    "rfe = RFE(estimator=LogisticRegression(max_iter=1000), n_features_to_select=2, step=1)\n",
    "rfe.fit(X, y)\n",
    "print(rfe.support_) \n",
    "print(rfe.ranking_)\n",
    "X_new = rfe.transform(X)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection using SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SelectFromModel` is a meta-transformer that can be used alongside any estimator that assigns importance to each feature through a specific attribute (such as `coef_`, `feature_importances_`) or via an `importance_getter` callable after fitting. The features are considered unimportant and removed if the corresponding importance of the feature values are below the provided threshold parameter. Apart from specifying the threshold numerically, there are built-in heuristics for finding a threshold using a string argument. Available heuristics are “mean”, “median” and float multiples of these like “0.1*mean”. In combination with the threshold criteria, one can use the `max_features` parameter to set a limit on the number of features to select."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: SVC Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(150, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "X, y = load_iris(return_X_y=True)\n",
    "print(X.shape)\n",
    "\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(X, y)\n",
    "model = SelectFromModel(lsvc, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">With SVMs and logistic-regression, the parameter C controls the sparsity: the smaller C the fewer features selected. With Lasso, the higher the alpha parameter, the fewer features selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Tree Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "[0.10769421 0.07282498 0.37546013 0.44402068]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(150, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "X, y = load_iris(return_X_y=True)\n",
    "print(X.shape)\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=50)\n",
    "clf = clf.fit(X, y)\n",
    "print(clf.feature_importances_)  \n",
    "\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "X_new.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential Feature Selection (SFS) is available in the `SequentialFeatureSelector` transformer. SFS can be either forward or backward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward SFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward-SFS is a greedy procedure that iteratively finds the best new feature to add to the set of selected features. Concretely, we initially start with zero feature and find the one feature that maximizes a cross-validated score when an estimator is trained on this single feature. Once that first feature is selected, we repeat the procedure by adding a new feature to the set of selected features. The procedure stops when the desired number of selected features is reached, as determined by the `n_features_to_select` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward SFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backward-SFS follows the same idea but works in the opposite direction: instead of starting with no feature and greedily adding features, we start with all the features and greedily remove features from the set. The `direction` parameter controls whether forward or backward SFS is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">In general, forward and backward selection do not yield equivalent results. Also, one may be much faster than the other depending on the requested number of selected features: if we have 10 features and ask for 7 selected features, forward selection would need to perform 7 iterations while backward selection would only need to perform 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SFS differs from `RFE` and `SelectFromModel` in that it does not require the underlying model to expose a `coef_` or `feature_importances_` attribute. It may however be slower considering that more models need to be evaluated, compared to the other approaches. For example in backward selection, the iteration going from *m* features to *m - 1* features using k-fold cross-validation requires fitting *m * k* models, while `RFE` would require only a single fit, and `SelectFromModel` always just does a single fit and requires no iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection as part of a pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection is usually used as a pre-processing step before doing the actual learning. The recommended way to do this in scikit-learn is to use a `Pipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('feature_selection',\n",
       "                 SelectFromModel(estimator=LinearSVC(penalty='l1'))),\n",
       "                ('classification', RandomForestClassifier())])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "clf = Pipeline([\n",
    "  ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\"))),\n",
    "  ('classification', RandomForestClassifier())\n",
    "])\n",
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `VarianceThreshold`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of `VarianceThreshold`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **threshold**: float, (default=0.0)\n",
    "  \n",
    "  Features with a training-set variance lower than this threshold will be removed. The default is to keep all features with non-zero variance, i.e. remove the features that have the same value in all samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of `VarianceThreshold`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **variances_**: array-like, shape (n_features,)\n",
    "    \n",
    "    The variance of each feature in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `SelectKBest`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of `SelectKBest`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **score_func**: callable, defualt=f_classif\n",
    "  \n",
    "    Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues) or a single array with scores. Default is f_classif. The default function only works with classification tasks.\n",
    "* **k**: int or \"all\", default=10\n",
    "    \n",
    "    Number of top features to select. The “all” option bypasses selection, for use in a parameter search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of `SelectKBest`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **scores_**: array-like, shape (n_features,)\n",
    "    \n",
    "    The scores associated with each feature.\n",
    "* **pvalues_**: array-like, shape (n_features,)\n",
    "    \n",
    "    The p-values associated with each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `SelectPercentile`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of `SelectPercentile`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **score_func**: callable, default=f_classif\n",
    "  \n",
    "    Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues) or a single array with scores. Default is f_classif. The default function only works with classification tasks.\n",
    "* **percentile**: int, default=10\n",
    "  \n",
    "    Percentile of features to keep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available callable score functions are:\n",
    "* For regression: f_regression, mutual_info_regression\n",
    "\n",
    "* For classification: chi2, f_classif, mutual_info_classif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of `SelectPercentile`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **scores_**: array-like, shape (n_features,)\n",
    "    \n",
    "    The scores associated with each feature.\n",
    "* **pvalues_**: array-like, shape (n_features,)\n",
    "    \n",
    "    The p-values associated with each feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `RFE`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of `RFE`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **estimator**: estimator object.\n",
    "  \n",
    "    A supervised learning estimator with a `fit` method that provides information about feature importance (e.g. `coef_`, `feature_importances_`).\n",
    "* **n_features_to_select**: int or float default=None\n",
    "  \n",
    "    The number of features to select. If None, half of the features are selected. If integer, the parameter is the absolute number of features to select. If float between 0 and 1, it is the fraction of features to select.\n",
    "* **step**: int or float, default=1\n",
    "  \n",
    "    If greater than or equal to 1, then `step` corresponds to the (integer) number of features to remove at each iteration. If within (0.0, 1.0), then `step` corresponds to the percentage (rounded down) of features to remove at each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of `RFE`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **classes_**: array-like, shape (n_classes,)\n",
    "    \n",
    "    Classes labels available when `estimator` is a classifier.\n",
    "* **estimator_**: estimator object.\n",
    "    \n",
    "    The underlying estimator.\n",
    "* **n_features_**: int\n",
    "  \n",
    "    The number of features.\n",
    "* **ranking_**: array-like, shape (n_features,)\n",
    "    \n",
    "    The feature ranking, such that `ranking_[i] `corresponds to the ranking position of the i-th feature. Selected (i.e., estimated best) features are assigned rank 1.\n",
    "* **support_**: array-like, shape (n_features,)\n",
    "        \n",
    "    The mask of selected features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `SelectFromModel`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of `SelectFromModel`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **estimator**: estimator object.\n",
    "  \n",
    "    The base estimator from which the transformer is built. This can be both a fitted (if `prefit` is set to `True`) or a non-fitted estimator. The estimator should have a `feature_importances_` or coef_ `attribute` after fitting. Otherwise, the `importance_getter` parameter should be used.\n",
    "* **threshold** : str or float, default=None\n",
    "\n",
    "    The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If “median” (resp. “mean”), then the threshold value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., “1.25*mean”) may also be used. If None and if the estimator has a parameter penalty set to l1, either explicitly or implicitly (e.g, Lasso), the threshold used is 1e-5. Otherwise, “mean” is used by default.\n",
    "* **prefit**: bool, default=False\n",
    "  \n",
    "    Whether a prefit model is expected to be passed into the constructor directly or not. If `True`, `estimator` must be a fitted estimator. If Fal`se, `estimator` is fitted and updated by calling `fit` and `partial_fit`, respectively.\n",
    "* **norm_order**: non-zero int, inf, -inf, default=1\n",
    "\n",
    "    Order of the norm used to filter the vectors of coefficients below threshold in the case where the `coef_` attribute of the estimator is of dimension 2.\n",
    "* **max_features** :int, callable, default=None\n",
    "\n",
    "    The maximum number of features to select.\n",
    "\n",
    "    If an integer, then it specifies the maximum number of features to allow.\n",
    "\n",
    "    If a callable, then it specifies how to calculate the maximum number of features allowed by using the output of `max_feaures(X)`.\n",
    "\n",
    "    If `None`, then all features are kept.\n",
    "    To only select based on `max_features`, set `threshold=-np.inf`\n",
    "* **importance_getter**: str or callable, default=’auto’\n",
    "\n",
    "    If ‘auto’, uses the feature importance either through a coef_ attribute or feature_importances_ attribute of estimator.\n",
    "\n",
    "    Also accepts a string that specifies an attribute name/path for extracting feature importance (implemented with attrgetter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of `SelectFromModel`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **estimator_**: estimator object.\n",
    "    \n",
    "    The base estimator from which the transformer is built. This attribute exist only when `fit` has been called.\n",
    "\n",
    "    * If `prefit=True`, it is a deep copy of estimator.\n",
    "\n",
    "    * If `prefit=False`, it is a clone of estimator and fit on the data passed to `fit` or `partial_fit`.\n",
    "* **max_features_**: int \n",
    "  \n",
    "    Maximum number of features calculated during fit. Only defined if the max_features is not None.\n",
    "\n",
    "    * If `max_features` is an int, then `max_features_ = max_features`.\n",
    "\n",
    "    * If `max_features` is a callable, then `max_features_ = max_features(X)`.\n",
    "* **threshold_** float\n",
    "\n",
    "    Threshold value used for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `SequentialFeatureSelector`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of `SequentialFeatureSelector`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **estimator**: estimator instance\n",
    "    \n",
    "    An unfitted estimator.\n",
    "\n",
    "* **n_features_to_select**: “auto”, int or float, default=’warn’\n",
    "  \n",
    "    If \"auto\", the behaviour depends on the `tol` parameter:\n",
    "\n",
    "    * if `tol` is not None, then features are selected until the score improvement does not exceed `tol`.\n",
    "\n",
    "    * otherwise, half of the features are selected.\n",
    "\n",
    "    If integer, the parameter is the absolute number of features to select. If float between 0 and 1, it is the fraction of features to select.\n",
    "* **tol**: float, default=None\n",
    "  \n",
    "  If the score is not incremented by at least tol between two consecutive feature additions or removals, stop adding or removing. tol is enabled only when `n_features_to_select` is \"auto\".\n",
    "* **direction**: {‘forward’, ‘backward’}, default=’forward’\n",
    "  \n",
    "    If ‘forward’, features are added. If ‘backward’, features are removed.\n",
    "* **scoring**:  str, callable, list/tuple or dict, default=None\n",
    "  \n",
    "    A single str or a callable to evaluate the predictions on the test set.\n",
    "\n",
    "    If None, the estimator’s score method is used.\n",
    "* **cv**: int, cross-validation generator or an iterable, default=None\n",
    "    \n",
    "    * Determines the cross-validation splitting strategy. Possible inputs for `cv` are:\n",
    "\n",
    "    * `None`, to use the default 5-fold cross validation,\n",
    "\n",
    "    * integer, to specify the number of folds in a `(Stratified)KFold`,\n",
    "\n",
    "    * CV splitter,\n",
    "\n",
    "    * An iterable yielding (train, test) splits as arrays of indices.\n",
    "* **n_jobs**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of `SequentialFeatureSelector`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **n_features_**: int\n",
    "  \n",
    "   Number of features seen during fit. Only defined if the underlying estimator exposes such an attribute when fit.\n",
    "* **n_features_to_select_**: int\n",
    "  \n",
    "   The number of features that were selected.\n",
    "* **support_**: array-like, shape (n_features,)\n",
    "  \n",
    "   The mask of selected features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('data-science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2efee1efa502125d01e6b4768ba06d9453d29f3642bfd14ad5d4a769de82e88c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
