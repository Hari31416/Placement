{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Contents\">Contents<a href=\"#Contents\"></a></h1>\n",
    "        <ol>\n",
    "        <li><a class=\"\" href=\"#Stochastic-Gradient-Descent\">Stochastic Gradient Descent</a></li>\n",
    "<ol><li><a class=\"\" href=\"#SGDClassifier\">SGDClassifier</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Loss-functions\">Loss functions</a></li>\n",
    "<li><a class=\"\" href=\"#Penalties\">Penalties</a></li>\n",
    "<li><a class=\"\" href=\"#Multi-class-classification\">Multi-class classification</a></li>\n",
    "<li><a class=\"\" href=\"#Parameters-of-SGDClassifier\">Parameters of SGDClassifier</a></li>\n",
    "<li><a class=\"\" href=\"#Attributes-of-SGDClassifier\">Attributes of SGDClassifier</a></li>\n",
    "</ol><li><a class=\"\" href=\"#SGDRegressor\">SGDRegressor</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Parameters-of-SGDRegressor\">Parameters of SGDRegressor</a></li>\n",
    "<li><a class=\"\" href=\"#Attributes-of-SGDRegressor\">Attributes of SGDRegressor</a></li>\n",
    "</ol><li><a class=\"\" href=\"#More-Informations-on-SGD\">More Informations on SGD</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Time-Complexity\">Time Complexity</a></li>\n",
    "<li><a class=\"\" href=\"#Stopping-Criterion\">Stopping Criterion</a></li>\n",
    "<li><a class=\"\" href=\"#Mathematical-Details\">Mathematical Details</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent (SGD) is a simple yet very efficient approach to fitting linear classifiers and regressors under convex loss functions such as (linear) Support Vector Machines and Logistic Regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD has been successfully applied to large-scale and sparse machine learning problems often encountered in text classification and natural language processing. Given that the data is sparse, the classifiers in this module easily scale to problems with more than $10^5$ training examples and more than $10^5$ features.\n",
    "\n",
    "Strictly speaking, SGD is merely an optimization technique and does not correspond to a specific family of machine learning models. It is only a way to train a model. Often, an instance of `SGDClassifier` or `SGDRegressor` will have an equivalent estimator in the scikit-learn API, potentially using a different optimization technique. For example, using `SGDClassifier(loss='log_loss')` results in logistic regression, i.e. a model equivalent to `LogisticRegression` which is fitted via SGD instead of being fitted by one of the other solvers in `LogisticRegression`. Similarly,` SGDRegressor(loss='squared_error', penalty='l2')` and `Ridge` solve the same optimization problem, via different means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantages of Stochastic Gradient Descent are:\n",
    "* Efficiency.\n",
    "\n",
    "* Ease of implementation (lots of opportunities for code tuning).\n",
    "\n",
    "The disadvantages of Stochastic Gradient Descent include:\n",
    "\n",
    "* SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations.\n",
    "\n",
    "* SGD is sensitive to feature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class SGDClassifier implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "X = [[0., 0.], [1., 1.]]\n",
    "y = [0, 1]\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=10)\n",
    "clf.fit(X, y)\n",
    "print(clf.predict([[2., 2.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD fits a linear model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[9.85221675, 9.85221675]]), array([-9.99002993]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_, clf.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The signed distance to the hyperplane (computed as the dot product between the coefficients and the input sample, plus the intercept) is given by `SGDClassifier.decision_function`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29.41883706])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.decision_function([[2., 2.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concrete loss function can be set via the loss parameter. `SGDClassifier` supports the following loss functions:\n",
    "\n",
    "* `loss=\"hinge\"`: (soft-margin) linear Support Vector Machine,\n",
    "\n",
    "* `loss=\"modified_huber\"`: smoothed hinge loss,\n",
    "\n",
    "* `loss=\"log\"`: logistic regression,\n",
    "\n",
    "and all regression losses. In this case the target is encoded as -1 or 1, and the problem is treated as a regression problem. The predicted class then correspond to the sign of the predicted target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two loss functions are lazy, they only update the model parameters if an example violates the margin constraint, which makes training very efficient and may result in sparser models (i.e. with more zero coefficients), even when L2 penalty is used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `loss=\"log\"` or `loss=\"modified_huber\"` enables the `predict_proba` method, which gives a vector of probability estimates $P(y|x)$ per sample $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00416343, 0.99583657]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SGDClassifier(loss=\"log\", max_iter=10).fit(X, y)\n",
    "clf.predict_proba([[1., 1.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concrete penalty can be set via the penalty parameter. SGD supports the following penalties:\n",
    "\n",
    "* **penalty=\"l2\"**: L2 norm penalty on `coef_`.\n",
    "\n",
    "* **penalty=\"l1\"**: L1 norm penalty on `coef_`.\n",
    "\n",
    "* **penalty=\"elasticnet\"**: Convex combination of L2 and L1; `(1 - l1_ratio) * L2 + l1_ratio * L1.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGDClassifier supports multi-class classification by combining multiple binary classifiers in a “one versus all” (OVA) scheme. For each of the $K$ classes, a binary classifier is learned that discriminates between that and all other $K-1$ classes. At testing time, we compute the confidence score (i.e. the signed distances to the hyperplane) for each classifier and choose the class with the highest confidence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Figure below illustrates the OVA approach on the iris dataset. The dashed lines represent the three OVA classifiers; the background colors show the decision surface induced by the three classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_sgd_iris_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of multi-class classification `coef_` is a two-dimensional array of shape (n_classes, n_features) and `intercept_` is a one-dimensional array of shape (n_classes,). The i-th row of `coef_` holds the weight vector of the OVA classifier for the i-th class; classes are indexed in ascending order. Note that, in principle, since they allow to create a probability model, loss=\"log_loss\" and loss=\"modified_huber\" are more suitable for one-vs-all classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the parameters of SGDClassifier:\n",
    "* **loss** : {‘hinge’, ‘log_loss’, ‘log’, ‘modified_huber’, ‘squared_hinge’, ‘perceptron’, ‘squared_error’, ‘huber’, ‘epsilon_insensitive’, ‘squared_epsilon_insensitive’}, default=’hinge’\n",
    " \n",
    "    The loss function to be used.\n",
    "\n",
    "    * ‘hinge’ gives a linear SVM.\n",
    "\n",
    "    * ‘log_loss’ gives logistic regression, a probabilistic classifier.\n",
    "\n",
    "    * ‘modified_huber’ is another smooth loss that brings tolerance to\n",
    "    outliers as well as probability estimates.\n",
    "\n",
    "    * ‘squared_hinge’ is like hinge but is quadratically penalized.\n",
    "\n",
    "    * ‘perceptron’ is the linear loss used by the perceptron algorithm.\n",
    "\n",
    "    * The other losses, ‘squared_error’, ‘huber’, ‘epsilon_insensitive’ and ‘squared_epsilon_insensitive’ are designed for regression but can be useful in classification as well.\n",
    "* **penalty** : {‘l2’, ‘l1’, ‘elasticnet’}, default=’l2’\n",
    " \n",
    "    The penalty (aka regularization term) to be used.\n",
    "\n",
    "    * ‘l2’ is the standard regularizer used in ridge regression.\n",
    "\n",
    "    * ‘l1’ is the standard regularizer used in lasso regression.\n",
    "\n",
    "    * ‘elasticnet’ is a combination of L1 and L2 regularization that is\n",
    "    also known as the elastic net.\n",
    "* **l_1_ratio** : float, optional, default=0.15\n",
    " \n",
    "    The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.\n",
    "* **fit_intercept** : bool, optional, default=True\n",
    "    \n",
    "    Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be centered).\n",
    "* **max_iter** : int, optional, default=1000\n",
    "\n",
    "    Maximum number of iterations for the solver. The solver iterates until convergence (determined by ‘tol’) or this number of iterations.\n",
    " \n",
    "    Maximum number of iterations for conjugate gradient solver.\n",
    "* **tol** : float, optional, default=1e-3\n",
    "    \n",
    "    Tolerance for stopping criteria.\n",
    "* **shuffle** : bool, optional, default=True\n",
    "  \n",
    "    Whether or not the training data should be shuffled before each epoch.\n",
    "* **epsilon** : float, optional, default=0.1\n",
    "    \n",
    "    Epsilon in the epsilon-insensitive loss functions; only if loss is ‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’. For ‘huber’, determines the threshold at which it becomes less important to get the prediction exactly right. For epsilon-insensitive, any differences between the current prediction and the correct label are ignored if they are less than this threshold. Values must be in the range [0.0, inf).\n",
    "\n",
    "* **alpha** : float, optional, default=0.0001\n",
    "    \n",
    "    Constant that multiplies the regularization term. The higher the value, the stronger the regularization. Also used to compute the learning rate when set to learning_rate is set to ‘optimal’. Values must be in the range [0.0, inf).\n",
    "\n",
    "* **learning_rate** : {‘constant’, ‘optimal’, ‘invscaling’, ‘adaptive’}, optional, default=’constant’\n",
    " \n",
    "    The learning rate schedule.\n",
    "    * ‘constant’: eta = eta0\n",
    "\n",
    "    * ‘optimal’: eta = 1.0 / (alpha * (t + t0)) where t0 is chosen by a heuristic proposed by Leon Bottou.\n",
    "\n",
    "    * ‘invscaling’: eta = eta0 / pow(t, power_t)\n",
    "\n",
    "    * ‘adaptive’: eta = eta0, as long as the training keeps decreasing. Each time n_iter_no_change consecutive epochs fail to decrease the training loss by tol or fail to increase validation score by tol if early_stopping is True, the current learning rate is divided by 5.\n",
    "* **eta0** : float, optional, default=0.0\n",
    "    \n",
    "    The initial learning rate for the ‘constant’ or ‘invscaling’ schedules. The default value is 0.0.\n",
    "* **power_t** : float, optional, default=0.5\n",
    "  \n",
    "    The exponent for inverse scaling learning rate. It is used only when learning_rate is set to ‘invscaling’.\n",
    "\n",
    "* **early_stopping** : bool, optional, default=False\n",
    "    \n",
    "    Whether to use early stopping to terminate training when validation score is not improving. If set to true, it will automatically set aside 10% of training data as validation and terminate training when validation score is not improving by at least tol for n_iter_no_change consecutive epochs.\n",
    "\n",
    "* **n_iter_no_change** : int, optional, default=5\n",
    "     \n",
    "    Number of iterations with no improvement after which training will be terminated.\n",
    "\n",
    "* **class_weight** : dict, {class_label: weight}, optional, default=None\n",
    "    \n",
    "    Weights associated with classes. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.\n",
    "    The ‘balanced’ mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data.\n",
    "    For multi-output, the weights are normalized separately for each column.\n",
    "* **warm_start** : bool, optional, default=False\n",
    "    \n",
    "    When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **coef_** : array, shape (1, n_features) or (n_classes, n_features)\n",
    "    \n",
    "    Weights assigned to the features.\n",
    "* **intercept_** : array, shape (1,) or (n_classes,)\n",
    "  \n",
    "    Constant bias assigned to the samples.\n",
    "* **n_iter_** : int\n",
    "        \n",
    "        Number of iterations run.\n",
    "* **classes_** : array of shape (n_classes,)\n",
    "    \n",
    "    The classes labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
    "Y = np.array([1, 1, 2, 2])\n",
    "clf = make_pipeline(StandardScaler(),\n",
    "                    SGDClassifier(max_iter=1000, tol=1e-3))\n",
    "clf.fit(X, Y)\n",
    "\n",
    "\n",
    "print(clf.predict([[-0.8, -1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `SGDRegressor` implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties to fit linear regression models. `SGDRegressor` is well suited for regression problems with a large number of training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concrete loss function can be set via the loss parameter. `SGDRegressor` supports the following loss functions:\n",
    "\n",
    "* **loss=\"squared_error\"**: Ordinary least squares,\n",
    "\n",
    "* **loss=\"huber\"**: Huber loss for robust regression,\n",
    "\n",
    "* **loss=\"epsilon_insensitive\"**: linear Support Vector Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of SGDRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SGDRegressor` class has the same parameters as the `SGDClassifier` class. Only the `loss` parameter is different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of SGDRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SGDRegressor` class has the same attributes as the `SGDClassifier` class except for the `classes_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('sgdregressor', SGDRegressor())])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "n_samples, n_features = 10, 5\n",
    "rng = np.random.RandomState(0)\n",
    "y = rng.randn(n_samples)\n",
    "X = rng.randn(n_samples, n_features)\n",
    "# Always scale the input. The most convenient way is to use a pipeline.\n",
    "reg = make_pipeline(StandardScaler(),\n",
    "                    SGDRegressor(max_iter=1000, tol=1e-3))\n",
    "reg.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Informations on SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The major advantage of SGD is its efficiency, which is basically linear in the number of training examples. If X is a matrix of size (n, p) training has a cost of $O(k n \\bar p)$\n",
    ", where k is the number of iterations (epochs) and \n",
    "$\\bar{p}$ is the average number of non-zero attributes per sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping Criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes `SGDClassifier` and `SGDRegressor` provide two criteria to stop the algorithm when a given level of convergence is reached:\n",
    "\n",
    "* With `early_stopping=True`, the input data is split into a training set and a validation set. The model is then fitted on the training set, and the stopping criterion is based on the prediction score (using the score method) computed on the validation set. The size of the validation set can be changed with the parameter `validation_fraction`.\n",
    "\n",
    "* With `early_stopping=False`, the model is fitted on the entire input data and the stopping criterion is based on the objective function computed on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases, the criterion is evaluated once by epoch, and the algorithm stops when the criterion does not improve `n_iter_no_change` times in a row. The improvement is evaluated with absolute tolerance `tol`, and the algorithm stops in any case after a maximum number of iteration `max_iter`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of training examples $(x_1, y_1), \\ldots, (x_n, y_n)$, our goal is to find a linear model $f(x) = w^T x + b$ which minimizes the following loss function:\n",
    "$$\n",
    "E(w,b) = \\frac{1}{n}\\sum_{i=1}^{n} L(y_i, f(x_i)) + \\alpha R(w)\n",
    "$$\n",
    "where $L$ is a loss function that measures model (mis)fit and $R$ is a regularization term (aka penalty) that penalizes model complexity; $\\alpha$ is a non-negative hyperparameter that controls the regularization strength.\n",
    "\n",
    "The choice of $L$ determines which loss function is used. The choice of $R$ determines the regularization term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic gradient descent is an optimization method for unconstrained optimization problems. In contrast to (batch) gradient descent, SGD approximates the true gradient of $E(w,b)$ by considering a single training example at a time.\n",
    "\n",
    "The class `SGDClassifier` implements a first-order SGD learning routine. The algorithm iterates over the training examples and for each example updates the model parameters according to the update rule given by:\n",
    "$$w \\leftarrow w - \\eta \\left[\\alpha \\frac{\\partial R(w)}{\\partial w}\n",
    "+ \\frac{\\partial L(w^T x_i + b, y_i)}{\\partial w}\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\eta$ is the learning rate which controls the step-size in the parameter space. The intercept $b$ is updated similarly but without regularization. The learning rate can be sheduled over the training epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "21040c1b576dca9f4f330277849b9f4819256d524dee23c2b89e431027dafe11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
