{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Contents\">Contents<a href=\"#Contents\"></a></h1>\n",
    "        <ol>\n",
    "        <li><a class=\"\" href=\"#Ensemble-Learning\">Ensemble Learning</a></li>\n",
    "<li><a class=\"\" href=\"#Average-Ensemble-Methods\">Average Ensemble Methods</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Bagging-meta-estimator\">Bagging meta-estimator</a></li>\n",
    "<li><a class=\"\" href=\"#Forests-of-randomized-trees\">Forests of randomized trees</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Random-Forest\">Random Forest</a></li>\n",
    "<li><a class=\"\" href=\"#Extremely-Randomized-Trees\">Extremely Randomized Trees</a></li>\n",
    "<li><a class=\"\" href=\"#Parameters\">Parameters</a></li>\n",
    "<li><a class=\"\" href=\"#Time-Complexity\">Time Complexity</a></li>\n",
    "<li><a class=\"\" href=\"#Parallelization\">Parallelization</a></li>\n",
    "<li><a class=\"\" href=\"#Feature-Importance\">Feature Importance</a></li>\n",
    "</ol><li><a class=\"\" href=\"#Voting-Classifier\">Voting Classifier</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Majority-Class-Labels-(Hard-Voting)\">Majority Class Labels (Hard Voting)</a></li>\n",
    "<li><a class=\"\" href=\"#Weighted-Average-Probabilities-(Soft-Voting)\">Weighted Average Probabilities (Soft Voting)</a></li>\n",
    "<li><a class=\"\" href=\"#Using-the-VotingClassifier-with-GridSearchCV\">Using the VotingClassifier with GridSearchCV</a></li>\n",
    "</ol><li><a class=\"\" href=\"#Voting-Regressor\">Voting Regressor</a></li>\n",
    "<li><a class=\"\" href=\"#Stacked-generalization\">Stacked generalization</a></li>\n",
    "</ol><li><a class=\"\" href=\"#Boosting-Methods\">Boosting Methods</a></li>\n",
    "<ol><li><a class=\"\" href=\"#AdaBoost\">AdaBoost</a></li>\n",
    "<li><a class=\"\" href=\"#Gradient-Tree-Boosting\">Gradient Tree Boosting</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Controlling-the-Tree-Size\">Controlling the Tree Size</a></li>\n",
    "<li><a class=\"\" href=\"#Loss-Functions\">Loss Functions</a></li>\n",
    "</ol><li><a class=\"\" href=\"#Histogram-Based-Gradient-Boosting\">Histogram-Based Gradient Boosting</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Missing-values-support\">Missing values support</a></li>\n",
    "<li><a class=\"\" href=\"#Sample-weight-support\">Sample weight support</a></li>\n",
    "<li><a class=\"\" href=\"#Monotonic-Constraints\">Monotonic Constraints</a></li>\n",
    "</ol><li><a class=\"\" href=\"#Algorithms\">Algorithms</a></li>\n",
    "<ol><li><a class=\"\" href=\"#BaggingClassifier\">BaggingClassifier</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Parameters-of-the-BaggingClassifier\">Parameters of the BaggingClassifier</a></li>\n",
    "<li><a class=\"\" href=\"#Attributes-of-the-BaggingClassifier\">Attributes of the BaggingClassifier</a></li>\n",
    "</ol><li><a class=\"\" href=\"#BaggingRegressor\">BaggingRegressor</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Parameters-of-the-BaggingRegressor\">Parameters of the BaggingRegressor</a></li>\n",
    "<li><a class=\"\" href=\"#Attributes-of-the-BaggingRegressor\">Attributes of the BaggingRegressor</a></li>\n",
    "</ol><li><a class=\"\" href=\"#RandomForestClassifier\">RandomForestClassifier</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Parameters-of-the-RandomForestClassifier\">Parameters of the RandomForestClassifier</a></li>\n",
    "<li><a class=\"\" href=\"#Attributes-of-the-RandomForestClassifier\">Attributes of the RandomForestClassifier</a></li>\n",
    "</ol><li><a class=\"\" href=\"#RandomForestRegressor\">RandomForestRegressor</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Parameters-of-the-RandomForestRegressor\">Parameters of the RandomForestRegressor</a></li>\n",
    "<li><a class=\"\" href=\"#Attributes-of-the-RandomForestRegressor\">Attributes of the RandomForestRegressor</a></li>\n",
    "</ol><li><a class=\"\" href=\"#ExtraTreesClassifier\">ExtraTreesClassifier</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Parameters-of-the-ExtraTreesClassifier\">Parameters of the ExtraTreesClassifier</a></li>\n",
    "<li><a class=\"\" href=\"#Attributes-of-the-ExtraTreesClassifier\">Attributes of the ExtraTreesClassifier</a></li>\n",
    "</ol><li><a class=\"\" href=\"#ExtraTreesRegressor\">ExtraTreesRegressor</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Parameters-of-the-ExtraTreesRegressor\">Parameters of the ExtraTreesRegressor</a></li>\n",
    "<li><a class=\"\" href=\"#Attributes-of-the-ExtraTreesRegressor\">Attributes of the ExtraTreesRegressor</a></li>\n",
    "</ol><li><a class=\"\" href=\"#VotingClassifier\">VotingClassifier</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Parameters-of-the-VotingClassifier\">Parameters of the VotingClassifier</a></li>\n",
    "<li><a class=\"\" href=\"#Attributes-of-the-VotingClassifier\">Attributes of the VotingClassifier</a></li>\n",
    "</ol><li><a class=\"\" href=\"#VotingRegressor\">VotingRegressor</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Parameters-of-the-VotingRegressor\">Parameters of the VotingRegressor</a></li>\n",
    "<li><a class=\"\" href=\"#Attributes-of-the-VotingRegressor\">Attributes of the VotingRegressor</a></li>\n",
    "</ol><li><a class=\"\" href=\"#StackingClassifier\">StackingClassifier</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Parameters-of-the-StackingClassifier\">Parameters of the StackingClassifier</a></li>\n",
    "<li><a class=\"\" href=\"#Attributes-of-the-StackingClassifier\">Attributes of the StackingClassifier</a></li>\n",
    "</ol><li><a class=\"\" href=\"#StackingRegressor\">StackingRegressor</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Parameters-of-the-StackingRegressor\">Parameters of the StackingRegressor</a></li>\n",
    "<li><a class=\"\" href=\"#Attributes-of-the-StackingRegressor\">Attributes of the StackingRegressor</a></li>\n",
    "</ol><li><a class=\"\" href=\"#AdaBoostClassifier\">AdaBoostClassifier</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Parameters-of-the-AdaBoostClassifier\">Parameters of the AdaBoostClassifier</a></li>\n",
    "<li><a class=\"\" href=\"#Attributes-of-the-AdaBoostClassifier\">Attributes of the AdaBoostClassifier</a></li>\n",
    "</ol><li><a class=\"\" href=\"#AdaBoostRegressor\">AdaBoostRegressor</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Parameters-of-the-AdaBoostRegressor\">Parameters of the AdaBoostRegressor</a></li>\n",
    "<li><a class=\"\" href=\"#Attributes-of-the-AdaBoostRegressor\">Attributes of the AdaBoostRegressor</a></li>\n",
    "</ol><li><a class=\"\" href=\"#GradientBoostingClassifier\">GradientBoostingClassifier</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Attributes-of-the-GradientBoostingClassifier\">Attributes of the GradientBoostingClassifier</a></li>\n",
    "</ol><li><a class=\"\" href=\"#GradientBoostingRegressor\">GradientBoostingRegressor</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Parameters-of-the-GradientBoostingRegressor\">Parameters of the GradientBoostingRegressor</a></li>\n",
    "<li><a class=\"\" href=\"#Attributes-of-the-GradientBoostingRegressor\">Attributes of the GradientBoostingRegressor</a></li>\n",
    "</ol><li><a class=\"\" href=\"#HistGradientBoostingClassifier\">HistGradientBoostingClassifier</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Parameters-of-the-HistGradientBoostingClassifier\">Parameters of the HistGradientBoostingClassifier</a></li>\n",
    "<li><a class=\"\" href=\"#Attributes-of-the-HistGradientBoostingClassifier\">Attributes of the HistGradientBoostingClassifier</a></li>\n",
    "</ol><li><a class=\"\" href=\"#HistGradientBoostingRegressor\">HistGradientBoostingRegressor</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Parameters-of-the-HistGradientBoostingRegressor\">Parameters of the HistGradientBoostingRegressor</a></li>\n",
    "<li><a class=\"\" href=\"#Attributes-of-the-HistGradientBoostingRegressor\">Attributes of the HistGradientBoostingRegressor</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of ensemble methods is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.\n",
    "\n",
    "Two families of ensemble methods are usually distinguished:\n",
    "\n",
    "* In averaging methods, the driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.\n",
    "\n",
    "    Examples: Bagging methods, Forests of randomized trees etc.\n",
    "\n",
    "* By contrast, in boosting methods, base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.\n",
    "\n",
    "    Examples: AdaBoost, Gradient Tree Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging meta-estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ensemble algorithms, bagging methods form a class of algorithms which build several instances of a black-box estimator on random subsets of the original training set and then aggregate their individual predictions to form a final prediction. These methods are used as a way to reduce the variance of a base estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">As they provide a way to reduce overfitting, bagging methods work best with strong and complex models (e.g., fully developed decision trees), in contrast with boosting methods which usually work best with weak models (e.g., shallow decision trees)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging methods come in many flavours but mostly differ from each other by the way they draw random subsets of the training set:\n",
    "\n",
    "* When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as **Pasting**.\n",
    "\n",
    "* When samples are drawn with replacement, then the method is known as **Bagging**.\n",
    "\n",
    "* When random subsets of the dataset are drawn as random subsets of the features, then the method is known as **Random Subspaces**.\n",
    "\n",
    "* Finally, when base estimators are built on subsets of both samples and features, then the method is known as **Random Patches**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn, bagging methods are offered as a unified `BaggingClassifier` meta-estimator (resp. `BaggingRegressor`), taking as input a user-specified base estimator along with parameters specifying the strategy to draw random subsets. In particular, `max_samples` and `max_features` control the size of the subsets (in terms of samples and features), while `bootstrap` and `bootstrap_features` control whether samples and features are drawn with or without replacement. When using a subset of the available samples the generalization accuracy can be estimated with the out-of-bag samples by setting `oob_score=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "bagging = BaggingClassifier(KNeighborsClassifier(),\n",
    "                            max_samples=0.5, max_features=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For details see the papers:\n",
    "\n",
    "[1](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.308.8562&rep=rep1&type=pdf)\n",
    "\n",
    "[2](http://www.cs.utsa.edu/~bylander/cs6243/breiman96bagging.pdf)\n",
    "\n",
    "[3](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=709601)\n",
    "\n",
    "[4](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.708.3190&rep=rep1&type=pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Forests of randomized trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sklearn.ensemble` module includes two averaging algorithms based on randomized decision trees: the RandomForest algorithm and the Extra-Trees method. Both algorithms are perturb-and-combine techniques specifically designed for trees. The prediction of the ensemble is given as the averaged prediction of the individual classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In random forests, each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set. Furthermore, when splitting each node during the construction of a tree, the best split is found either from all input features or a random subset of size `max_features`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of these two sources of randomness is to decrease the variance of the forest estimator. The injected randomness in forests yield decision trees with somewhat decoupled prediction errors. By taking an average of those predictions, some errors can cancel out. Random forests achieve a reduced variance by combining diverse trees, sometimes at the cost of a slight increase in bias.\n",
    "\n",
    "For details, see the [paper](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn, the `RandomForestClassifier` and `RandomForestRegressor` estimators are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extremely Randomized Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In extremely randomized trees, randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9823000000000001"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X, y = make_blobs(n_samples=10000, n_features=10, centers=100,\n",
    "    random_state=0)\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,\n",
    "    random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9997"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "    min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,\n",
    "    min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main parameters to adjust when using these methods is `n_estimators` and `max_features`. The former is the number of trees in the forest. The larger the better, but also the longer it will take to compute. In addition, note that results will stop getting significantly better beyond a critical number of trees. The latter is the size of the random subsets of features to consider when splitting a node. The lower the greater the reduction of variance, but also the greater the increase in bias. Empirical good default values are `max_features=1.0 `or equivalently `max_features=None` (always considering all features instead of a random subset) for regression problems, and `max_features=\"sqrt\"` (using a random subset of size `sqrt(n_features)`) for classification tasks (where n_features is the number of features in the data). The default value of `max_features=1.0` is equivalent to bagged trees and more randomness can be achieved by setting smaller values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good results are often achieved when setting `max_depth=None` in combination with `min_samples_split=2` (i.e., when fully developing the trees). When using bootstrap sampling the generalization error can be estimated on the left out or out-of-bag samples. This can be enabled by setting `oob_score=True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the model with the default parameters is $O( M * N * log (N) )$, where $M$ is the number of trees and $N$ is the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If `n_jobs=k` then computations are partitioned into `k` jobs, and run on `k` cores of the machine. If `n_jobs=-1` then all cores available on the machine are used. Note that because of inter-process communication overhead, the speedup might not be linear (i.e., using `k` jobs will unfortunately not be `k` times as fast). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relative rank (i.e. depth) of a feature used as a decision node in a tree can be used to assess the relative importance of that feature with respect to the predictability of the target variable. Features used at the top of the tree contribute to the final prediction decision of a larger fraction of the input samples. The expected fraction of the samples they contribute to can thus be used as an estimate of the relative importance of the features. In scikit-learn, the fraction of samples a feature contributes to is combined with the decrease in impurity from splitting them to create a normalized estimate of the predictive power of that feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice those estimates are stored as an attribute named `feature_importances_` on the fitted model. This is an array with shape `(n_features,)` whose values are positive and sum to 1.0. The higher the value, the more important is the contribution of the matching feature to the prediction function.\n",
    "\n",
    "You can have a look at this [paper](https://arxiv.org/pdf/1407.7502.pdf) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind the `VotingClassifier` is to combine conceptually different machine learning classifiers and use a majority vote or the average predicted probabilities (soft vote) to predict the class labels. Such a classifier can be useful for a set of equally well performing model in order to balance out their individual weaknesses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Majority Class Labels (Hard Voting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In majority voting, the predicted class label for a particular sample is the class label that represents the majority (mode) of the class labels predicted by each individual classifier. This can be done by using `voting='hard'` in the `VotingClassifier` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95 (+/- 0.04) [Logistic Regression]\n",
      "Accuracy: 0.94 (+/- 0.04) [Random Forest]\n",
      "Accuracy: 0.91 (+/- 0.04) [naive Bayes]\n",
      "Accuracy: 0.95 (+/- 0.04) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[:, 1:3], iris.target\n",
    "\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(n_estimators=50, random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "\n",
    "eclf = VotingClassifier(\n",
    "    estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
    "    voting='hard')\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X, y, scoring='accuracy', cv=5)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Weighted Average Probabilities (Soft Voting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to majority voting (hard voting), soft voting returns the class label as argmax of the sum of predicted probabilities.\n",
    "\n",
    "Specific weights can be assigned to each classifier via the weights parameter. When weights are provided, the predicted class probabilities for each classifier are collected, multiplied by the classifier weight, and averaged. The final class label is then derived from the class label with the highest average probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95 (+/- 0.03) [Decision Tree]\n",
      "Accuracy: 0.94 (+/- 0.04) [Neighnours]\n",
      "Accuracy: 0.95 (+/- 0.03) [SVM]\n",
      "Accuracy: 0.93 (+/- 0.07) [Ensemble]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from itertools import product\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Loading some example data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [0, 2]]\n",
    "y = iris.target\n",
    "\n",
    "# Training classifiers\n",
    "clf1 = DecisionTreeClassifier(max_depth=4)\n",
    "clf2 = KNeighborsClassifier(n_neighbors=7)\n",
    "clf3 = SVC(kernel='rbf', probability=True)\n",
    "eclf = VotingClassifier(estimators=[('dt', clf1), ('knn', clf2), ('svc', clf3)],\n",
    "                        voting='soft', weights=[2, 1, 2])\n",
    "\n",
    "for clf, label in zip([clf1, clf2, clf3, eclf], ['Decision Tree', 'Neighnours', 'SVM', 'Ensemble']):\n",
    "    scores = cross_val_score(clf, X, y, scoring='accuracy', cv=5)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the `VotingClassifier` with `GridSearchCV`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `VotingClassifier` can also be used together with `GridSearchCV` in order to tune the hyperparameters of the individual estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(random_state=1)\n",
    "clf3 = GaussianNB()\n",
    "eclf = VotingClassifier(\n",
    "    estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "params = {'lr__C': [1.0, 100.0], 'rf__n_estimators': [20, 200]}\n",
    "\n",
    "grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\n",
    "grid = grid.fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind the `VotingRegressor` is to combine conceptually different machine learning regressors and return the average predicted values. Such a regressor can be useful for a set of equally well performing models in order to balance out their individual weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# Loading some example data\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "\n",
    "# Training classifiers\n",
    "reg1 = GradientBoostingRegressor(random_state=1)\n",
    "reg2 = RandomForestRegressor(random_state=1)\n",
    "reg3 = LinearRegression()\n",
    "ereg = VotingRegressor(estimators=[('gb', reg1), ('rf', reg2), ('lr', reg3)])\n",
    "ereg = ereg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_voting_regressor_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacked generalization is a method for combining estimators to reduce their biases. More precisely, the predictions of each individual estimator are stacked together and used as input to a final estimator to compute the prediction. This final estimator is trained through cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `StackingClassifier` and `StackingRegressor` provide such strategies which can be applied to classification and regression problems.\n",
    "\n",
    "The `estimators` parameter corresponds to the list of the estimators which are stacked together in parallel on the input data. It should be given as a list of names and estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "estimators = [('ridge', RidgeCV()),\n",
    "              ('lasso', LassoCV(random_state=42)),\n",
    "              ('knr', KNeighborsRegressor(n_neighbors=20,\n",
    "                                          metric='euclidean'))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `final_estimator` will use the predictions of the estimators as input. It needs to be a classifier or a regressor when using `StackingClassifier` or `StackingRegressor`, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "final_estimator = GradientBoostingRegressor(\n",
    "    n_estimators=25, subsample=0.5, min_samples_leaf=25, max_features=1,\n",
    "    random_state=42)\n",
    "reg = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=final_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StackingRegressor(estimators=[(&#x27;ridge&#x27;, RidgeCV()),\n",
       "                              (&#x27;lasso&#x27;, LassoCV(random_state=42)),\n",
       "                              (&#x27;knr&#x27;,\n",
       "                               KNeighborsRegressor(metric=&#x27;euclidean&#x27;,\n",
       "                                                   n_neighbors=20))],\n",
       "                  final_estimator=GradientBoostingRegressor(max_features=1,\n",
       "                                                            min_samples_leaf=25,\n",
       "                                                            n_estimators=25,\n",
       "                                                            random_state=42,\n",
       "                                                            subsample=0.5))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StackingRegressor</label><div class=\"sk-toggleable__content\"><pre>StackingRegressor(estimators=[(&#x27;ridge&#x27;, RidgeCV()),\n",
       "                              (&#x27;lasso&#x27;, LassoCV(random_state=42)),\n",
       "                              (&#x27;knr&#x27;,\n",
       "                               KNeighborsRegressor(metric=&#x27;euclidean&#x27;,\n",
       "                                                   n_neighbors=20))],\n",
       "                  final_estimator=GradientBoostingRegressor(max_features=1,\n",
       "                                                            min_samples_leaf=25,\n",
       "                                                            n_estimators=25,\n",
       "                                                            random_state=42,\n",
       "                                                            subsample=0.5))</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>ridge</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RidgeCV</label><div class=\"sk-toggleable__content\"><pre>RidgeCV()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lasso</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LassoCV</label><div class=\"sk-toggleable__content\"><pre>LassoCV(random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>knr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsRegressor</label><div class=\"sk-toggleable__content\"><pre>KNeighborsRegressor(metric=&#x27;euclidean&#x27;, n_neighbors=20)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(max_features=1, min_samples_leaf=25, n_estimators=25,\n",
       "                          random_state=42, subsample=0.5)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "StackingRegressor(estimators=[('ridge', RidgeCV()),\n",
       "                              ('lasso', LassoCV(random_state=42)),\n",
       "                              ('knr',\n",
       "                               KNeighborsRegressor(metric='euclidean',\n",
       "                                                   n_neighbors=20))],\n",
       "                  final_estimator=GradientBoostingRegressor(max_features=1,\n",
       "                                                            min_samples_leaf=25,\n",
       "                                                            n_estimators=25,\n",
       "                                                            random_state=42,\n",
       "                                                            subsample=0.5))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "X, y = load_diabetes(return_X_y=True)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state=42)\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, the estimators are fitted on the whole training data `X_train`. They will be used when calling predict or `predict_proba`. To generalize and avoid over-fitting, the `final_estimator` is trained on out-samples using sklearn.`model_selection.cross_val_predict` internally.\n",
    "\n",
    "For `StackingClassifier`, note that the output of the estimators is controlled by the parameter `stack_method` and it is called by each estimator. This parameter is either a string, being estimator method names, or `'auto'` which will automatically identify an available method depending on the availability, tested in the order of preference: `predict_proba`, `decision_function` and `predict`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that it is also possible to get the output of the stacked `estimators` using the `transform` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[142.36209608, 138.30724927, 146.1       ],\n",
       "       [179.700576  , 182.89812552, 151.75      ],\n",
       "       [139.89817956, 132.46803343, 158.25      ],\n",
       "       [286.95180286, 292.65695767, 225.4       ],\n",
       "       [126.88317154, 124.1215975 , 164.65      ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.transform(X_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">In practice, a stacking predictor predicts as good as the best predictor of the base layer and even sometimes outperforms it by combining the different strengths of the these predictors. However, training a stacking predictor is computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Multiple stacking layers can be achieved by assigning `final_estimator` to a `StackingClassifier` or `StackingRegressor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StackingRegressor(estimators=[(&#x27;ridge&#x27;, RidgeCV()),\n",
       "                              (&#x27;lasso&#x27;, LassoCV(random_state=42)),\n",
       "                              (&#x27;knr&#x27;,\n",
       "                               KNeighborsRegressor(metric=&#x27;euclidean&#x27;,\n",
       "                                                   n_neighbors=20))],\n",
       "                  final_estimator=StackingRegressor(estimators=[(&#x27;rf&#x27;,\n",
       "                                                                 RandomForestRegressor(max_features=1,\n",
       "                                                                                       max_leaf_nodes=5,\n",
       "                                                                                       n_estimators=10,\n",
       "                                                                                       random_state=42)),\n",
       "                                                                (&#x27;gbrt&#x27;,\n",
       "                                                                 GradientBoostingRegressor(max_features=1,\n",
       "                                                                                           max_leaf_nodes=5,\n",
       "                                                                                           n_estimators=10,\n",
       "                                                                                           random_state=42))],\n",
       "                                                    final_estimator=RidgeCV()))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StackingRegressor</label><div class=\"sk-toggleable__content\"><pre>StackingRegressor(estimators=[(&#x27;ridge&#x27;, RidgeCV()),\n",
       "                              (&#x27;lasso&#x27;, LassoCV(random_state=42)),\n",
       "                              (&#x27;knr&#x27;,\n",
       "                               KNeighborsRegressor(metric=&#x27;euclidean&#x27;,\n",
       "                                                   n_neighbors=20))],\n",
       "                  final_estimator=StackingRegressor(estimators=[(&#x27;rf&#x27;,\n",
       "                                                                 RandomForestRegressor(max_features=1,\n",
       "                                                                                       max_leaf_nodes=5,\n",
       "                                                                                       n_estimators=10,\n",
       "                                                                                       random_state=42)),\n",
       "                                                                (&#x27;gbrt&#x27;,\n",
       "                                                                 GradientBoostingRegressor(max_features=1,\n",
       "                                                                                           max_leaf_nodes=5,\n",
       "                                                                                           n_estimators=10,\n",
       "                                                                                           random_state=42))],\n",
       "                                                    final_estimator=RidgeCV()))</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>ridge</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RidgeCV</label><div class=\"sk-toggleable__content\"><pre>RidgeCV()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>lasso</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LassoCV</label><div class=\"sk-toggleable__content\"><pre>LassoCV(random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>knr</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsRegressor</label><div class=\"sk-toggleable__content\"><pre>KNeighborsRegressor(metric=&#x27;euclidean&#x27;, n_neighbors=20)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>rf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(max_features=1, max_leaf_nodes=5, n_estimators=10,\n",
       "                      random_state=42)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>gbrt</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor(max_features=1, max_leaf_nodes=5, n_estimators=10,\n",
       "                          random_state=42)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RidgeCV</label><div class=\"sk-toggleable__content\"><pre>RidgeCV()</pre></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "StackingRegressor(estimators=[('ridge', RidgeCV()),\n",
       "                              ('lasso', LassoCV(random_state=42)),\n",
       "                              ('knr',\n",
       "                               KNeighborsRegressor(metric='euclidean',\n",
       "                                                   n_neighbors=20))],\n",
       "                  final_estimator=StackingRegressor(estimators=[('rf',\n",
       "                                                                 RandomForestRegressor(max_features=1,\n",
       "                                                                                       max_leaf_nodes=5,\n",
       "                                                                                       n_estimators=10,\n",
       "                                                                                       random_state=42)),\n",
       "                                                                ('gbrt',\n",
       "                                                                 GradientBoostingRegressor(max_features=1,\n",
       "                                                                                           max_leaf_nodes=5,\n",
       "                                                                                           n_estimators=10,\n",
       "                                                                                           random_state=42))],\n",
       "                                                    final_estimator=RidgeCV()))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_layer_rfr = RandomForestRegressor(\n",
    "    n_estimators=10, max_features=1, max_leaf_nodes=5,random_state=42)\n",
    "final_layer_gbr = GradientBoostingRegressor(\n",
    "    n_estimators=10, max_features=1, max_leaf_nodes=5,random_state=42)\n",
    "final_layer = StackingRegressor(\n",
    "    estimators=[('rf', final_layer_rfr),\n",
    "                ('gbrt', final_layer_gbr)],\n",
    "    final_estimator=RidgeCV()\n",
    "    )\n",
    "multi_layer_regressor = StackingRegressor(\n",
    "    estimators=[('ridge', RidgeCV()),\n",
    "                ('lasso', LassoCV(random_state=42)),\n",
    "                ('knr', KNeighborsRegressor(n_neighbors=20,\n",
    "                                            metric='euclidean'))],\n",
    "    final_estimator=final_layer\n",
    ")\n",
    "multi_layer_regressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.53\n"
     ]
    }
   ],
   "source": [
    "multi_layer_regressor.fit(X_train, y_train)\n",
    "\n",
    "print('R2 score: {:.2f}'\n",
    "      .format(multi_layer_regressor.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module sklearn.ensemble includes the popular boosting algorithm `AdaBoost`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consist of applying weights $w_1, w_2, \\ldots, w_N$  to each of the training samples. Initially, those weights are all set to $1/N$, so that the first step simply trains a weak learner on the original data. For each successive iteration, the sample weights are individually modified and the learning algorithm is reapplied to the reweighted data. At a given step, those training examples that were incorrectly predicted by the boosted model induced at the previous step have their weights increased, whereas the weights are decreased for those that were predicted correctly. As iterations proceed, examples that are difficult to predict receive ever-increasing influence. Each subsequent weak learner is thereby forced to concentrate on the examples that are missed by the previous ones in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_adaboost_hastie_10_2_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost can be used both for classification and regression problems:\n",
    "\n",
    "* For multi-class classification, `AdaBoostClassifier` implements AdaBoost-SAMME and AdaBoost-SAMME.R.\n",
    "    The paper can be found [here](https://hastie.su.domains/Papers/SII-2-3-A8-Zhu.pdf).\n",
    "\n",
    "* For regression, `AdaBoostRegressor` implements AdaBoost.R2.\n",
    "    The paper can be found [here](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.31.314&rep=rep1&type=pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of weak learners is controlled by the parameter `n_estimators`. The `learning_rate` parameter controls the contribution of the weak learners in the final combination. By default, weak learners are decision stumps. Different weak learners can be specified through the `base_estimator` parameter. The main parameters to tune to obtain good results are `n_estimators` and the complexity of the base estimators (e.g., its depth `max_depth` or minimum required number of samples to consider a split `min_samples_split`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Tree Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Tree Boosting or Gradient Boosted Decision Trees (GBDT) is a generalization of boosting to arbitrary differentiable loss function. GBDT is an accurate and effective off-the-shelf procedure that can be used for both regression and classification problems in a variety of areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module `sklearn.ensemble` provides methods for both classification and regression via gradient boosted decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of weak learners (i.e. regression trees) is controlled by the parameter `n_estimators`; The size of each tree can be controlled either by setting the tree depth via `max_depth` or by setting the number of leaf nodes via `max_leaf_nodes`. The `learning_rate` is a hyper-parameter in the range (0.0, 1.0] that controls overfitting via shrinkage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.009154859960321"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "est = GradientBoostingRegressor(\n",
    "    n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0,\n",
    "    loss='squared_error'\n",
    ").fit(X_train, y_train)\n",
    "mean_squared_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `GradientBoostingRegressor` and `GradientBoostingClassifier` support `warm_start=True` which allows you to add more estimators to an already fitted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.840234741105356"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ = est.set_params(n_estimators=200, warm_start=True)  # set warm_start and new nr of trees\n",
    "_ = est.fit(X_train, y_train) # fit additional 100 trees to est\n",
    "mean_squared_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling the Tree Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the trees can be controlled either by setting the tree depth via `max_depth` or by setting the number of leaf nodes via `max_leaf_nodes`.\n",
    "\n",
    "If you specify `max_depth=h` then complete binary trees of depth `h` will be grown. Such trees will have (at most) `2**h` leaf nodes and `2**h - 1` split nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can control the tree size by specifying the number of leaf nodes via the parameter `max_leaf_nodes`. In this case, trees will be grown using best-first search where nodes with the highest improvement in impurity will be expanded first. A tree with `max_leaf_nodes=k` has k` - 1` split nodes and thus can model interactions of up to order `max_leaf_nodes - 1` ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually `max_leaf_nodes=k` gives comparable results to `max_depth=k-1` but is significantly faster to train at the expense of a slightly higher training error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loss functions are supported and can be specified using the parameter `loss`:\n",
    "\n",
    "1. Regression\n",
    "\n",
    "    - Squared error (`'squared_error'`): The natural choice for regression due to its superior computational properties. The initial model is given by the mean of the target values.\n",
    "\n",
    "    - Least absolute deviation (`'lad'`): A robust loss function for regression. The initial model is given by the median of the target values.\n",
    "\n",
    "    - Huber (`'huber'`): Another robust loss function that combines least squares and least absolute deviation; use `alpha` to control the sensitivity with regards to outliers.\n",
    "\n",
    "    - Quantile (`'quantile'`): A loss function for quantile regression. Use `0 < alpha < 1` to specify the quantile. This loss function can be used to create prediction intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Classification\n",
    "\n",
    "    - Binary log-loss (`'log-loss'`): The binomial negative log-likelihood loss function for binary classification. It provides probability estimates. The initial model is given by the log odds-ratio.\n",
    "\n",
    "    - Multi-class log-loss (`'log-loss'`): The multinomial negative log-likelihood loss function for multi-class classification with `n_classes` mutually exclusive classes. It provides probability estimates. The initial model is given by the prior probability of each class. At each iteration `n_classes` regression trees have to be constructed which makes GBRT rather inefficient for data sets with a large number of classes.\n",
    "\n",
    "    - Exponential loss (`'exponential'`): The same loss function as `AdaBoostClassifier`. Less robust to mislabeled examples than `'log-loss'`; can only be used for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Histogram-Based Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn provides a histogram-based gradient boosting method called `HistGradientBoostingRegressor` or `HistGradientBoostingClassifier` based on LightGBM.\n",
    "\n",
    "These histogram-based estimators can be orders of magnitude faster than GradientBoostingClassifier and GradientBoostingRegressor when the number of samples is larger than tens of thousands of samples.\n",
    "\n",
    "They also have built-in support for missing values, which avoids the need for an imputer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These fast estimators first bin the input samples X into integer-valued bins (typically 256 bins) which tremendously reduces the number of splitting points to consider, and allows the algorithm to leverage integer-based data structures (histograms) instead of relying on sorted continuous values when building the trees. The API of these estimators is slightly different, and some of the features from `GradientBoostingClassifier` and `GradientBoostingRegressor` are not yet supported, for instance some loss functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `n_estimators` parameters is changed with `max_iter` while most of the other parameters are the same as `GradientBoostingClassifier` and `GradientBoostingRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8965"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = HistGradientBoostingClassifier(max_iter=100).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8715"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available losses for regression are ‘squared_error’, ‘absolute_error’, which is less sensitive to outliers, and ‘poisson’, which is well suited to model counts and frequencies. For classification, ‘log_loss’ is the only option. For binary classification it uses the binary log loss, also kown as binomial deviance or binary cross-entropy. For `n_classes >= 3`, it uses the multi-class log loss function, with multinomial deviance and categorical cross-entropy as alternative names. The appropriate loss version is selected based on `y` passed to `fit`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the trees can be controlled through the `max_leaf_nodes`, `max_depth`, and `min_samples_leaf` parameters.\n",
    "\n",
    "The number of bins used to bin the data is controlled with the `max_bins` parameter. Using less bins acts as a form of regularization. It is generally recommended to use as many bins as possible, which is the default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`HistGradientBoostingClassifier` and `HistGradientBoostingRegressor` have built-in support for missing values (NaNs).\n",
    "\n",
    "During training, the tree grower learns at each split point whether samples with missing values should go to the left or right child, based on the potential gain. When predicting, samples with missing values are assigned to the left or right child consequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([0, np.nan, 1, 2, np.nan]).reshape(-1, 1)\n",
    "y = [0, 1, 0, 0, 1]\n",
    "gbdt = HistGradientBoostingClassifier(min_samples_leaf=1,\n",
    "                                      max_depth=2,\n",
    "                                      learning_rate=1,\n",
    "                                      max_iter=1).fit(X, y)\n",
    "gbdt.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no missing values were encountered for a given feature during training, then samples with missing values are mapped to whichever child has the most samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample weight support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`HistGradientBoostingClassifier` and `HistGradientBoostingRegressor` sample support weights during fit.\n",
    "\n",
    "The following toy example demonstrates how the model ignores the samples with zero sample weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9990209190235209"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[1, 0],\n",
    "     [1, 0],\n",
    "     [1, 0],\n",
    "     [0, 1]]\n",
    "y = [0, 0, 1, 0]\n",
    "# ignore the first 2 training samples by setting their weight to 0\n",
    "sample_weight = [0, 0, 1, 1]\n",
    "gb = HistGradientBoostingClassifier(min_samples_leaf=1)\n",
    "gb.fit(X, y, sample_weight=sample_weight)\n",
    "\n",
    "print(gb.predict([[1, 0]]))\n",
    "\n",
    "gb.predict_proba([[1, 0]])[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the `[1, 0]` is comfortably classified as 1 since the first two samples are ignored due to their sample weights.\n",
    "\n",
    "> Implementation detail: taking sample weights into account amounts to multiplying the gradients (and the hessians) by the sample weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monotonic Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the problem at hand, you may have prior knowledge indicating that a given feature should in general have a positive (or negative) effect on the target value. For example, all else being equal, a higher credit score should increase the probability of getting approved for a loan. Monotonic constraints allow you to incorporate such prior knowledge into the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A positive monotonic constraint is a constraint of the form:\n",
    "$$x_1 \\leq x_1' \\implies F(x_1, x_2) \\leq F(x_1', x_2)$$\n",
    "where  is the predictor with two features.\n",
    "\n",
    "Similarly, a negative monotonic constraint is of the form:\n",
    "$$x_1 \\leq x_1' \\implies F(x_1, x_2) \\geq F(x_1', x_2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can specify a monotonic constraint on each feature using the `monotonic_cst` parameter. For each feature, a value of 0 indicates no constraint, while -1 and 1 indicate a negative and positive constraint, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "# positive, negative, and no constraint on the 3 features\n",
    "gbdt = HistGradientBoostingRegressor(monotonic_cst=[1, -1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">In a binary classification context, imposing a monotonic constraint means that the feature is supposed to have a positive / negative effect on the probability to belong to the positive class. Monotonic constraints are not supported for multiclass context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `BaggingClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of the `BaggingClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the parameters of the `BaggingClassifier`:\n",
    "* **base_estimator**: object, default=None\n",
    "  \n",
    "    The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a `DecisionTreeClassifier`.\n",
    "* **n_estimators**: int, default=10\n",
    "  \n",
    "    The number of base estimators in the ensemble.\n",
    "* **max_samples**: int or float, default=1.0\n",
    "    \n",
    "    The number of samples to draw from X to train each base estimator (with replacement by default).\n",
    "\n",
    "    - If int, then draw `max_samples` samples.\n",
    "\n",
    "    - If float, then draw `max_samples * X.shape[0]` samples.\n",
    "* **max_features**: int or float, default=1.0\n",
    "  \n",
    "    The number of features to draw from X to train each base estimator (with replacement by default).\n",
    "\n",
    "    - If int, then draw `max_features` features.\n",
    "\n",
    "    - If float, then draw `max_features * X.shape[1]` features.\n",
    "* **bootstrap**: boolean, default=True\n",
    "  \n",
    "    If True, bootstrap samples are used to train each base estimator.\n",
    "* **bootstrap_features**: boolean, default=False\n",
    "\n",
    "    If True, bootstrap samples are used to train each base estimator.\n",
    "* **oob_score**: boolean, default=False\n",
    "  \n",
    "    If True, the base estimators are trained on the out-of-bag samples. Only available if `bootstrap=True`.\n",
    "* **warm_start**: boolean, default=False\n",
    "  \n",
    "    When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of the `BaggingClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the attributes of the `BaggingClassifier`:\n",
    "* **estimators_**: list of estimators\n",
    "  \n",
    "    The collection of fitted base estimators.\n",
    "* **base_estimator_**: estimator\n",
    "  \n",
    "    The base estimator from which the ensemble is grown.\n",
    "* **n_features_**: int\n",
    "  \n",
    "    The number of features when `fit` is performed.\n",
    "* **n_classes_**: int\n",
    "  \n",
    "    The number of classes when `fit` is performed.\n",
    "* **classes_**: ndarray of shape (n_classes,)\n",
    "  \n",
    "    The classes labels.\n",
    "* **estimators_samples_**: array-like of shape [n_estimators, n_samples]\n",
    "  \n",
    "    The subset of drawn samples (i.e., the training samples) for each base estimator.\n",
    "* **estimators_features_**: array-like of shape [n_estimators, n_features]\n",
    "  \n",
    "    The subset of drawn features for each base estimator.\n",
    "* **oob_score_**: float\n",
    "  \n",
    "    Score of the training dataset obtained using an out-of-bag estimate.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `BaggingRegressor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of the `BaggingRegressor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as `BaggingClassifier`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of the `BaggingRegressor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as `BaggingClassifier` but excluding the classes related attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `RandomForestClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of the `RandomForestClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the parameters of the `RandomForestClassifier`:\n",
    "* **n_estimators**: int, default=10\n",
    "  \n",
    "    The number of trees in the forest.\n",
    "* **criterion**: {“gini”, “entropy”, “log_loss”}, default=”gini”\n",
    "    \n",
    "    The function to measure the quality of a split. Supported criteria are \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
    "* **max_depth**: int or None, default=None\n",
    "  \n",
    "    The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "* **max_leaf_nodes**: int or None, default=None\n",
    "  \n",
    "    Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.\n",
    "* **splitter**: {“best”, “random”}, default=”best”\n",
    "  \n",
    "    The strategy used to choose the best split. Valid options are “best” to choose the best split and “random” to choose at random.\n",
    "* **min_samples_split**: int, default=2\n",
    "  \n",
    "    The minimum number of samples required to split an internal node.\n",
    "    - If int, then consider `min_samples_split` as the minimum number.\n",
    "\n",
    "    - If float, then `min_samples_split` is a fraction and `ceil(min_samples_split * n_samples)` are the minimum number of samples for each split.\n",
    "* **min_samples_leaf**: int, default=1\n",
    "  \n",
    "    The minimum number of samples required to be at a leaf node.\n",
    "    - If int, then consider `min_samples_leaf` as the minimum number.\n",
    "\n",
    "    - If float, then `min_samples_leaf` is a fraction and `ceil(min_samples_leaf * n_samples)` are the minimum number of samples for each node.\n",
    "* **min_weight_fraction_leaf**: float, default=0.0\n",
    "  \n",
    "    The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node.\n",
    "* **max_features**: “sqrt”, “log2”, None}, int or float, default=”sqrt”\n",
    "  \n",
    "    The number of features to consider when looking for the best split:\n",
    "    - If int, then consider `max_features` features at each split.\n",
    "    - If float, then `max_features` is a fraction and `int(max_features * n_features)` features are considered at each split.\n",
    "    - If “auto”, then `max_features=sqrt(n_features)`.\n",
    "    - If “sqrt”, then `max_features=sqrt(n_features)`.\n",
    "    - If “log2”, then `max_features=log2(n_features)`.\n",
    "    - If None, then `max_features=n_features`.\n",
    "* **max_leaf_nodes**: int or None, default=None\n",
    "  \n",
    "    Grow trees with `max_leaf_nodes` in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.\n",
    "* **bootstrap**: boolean, default=True\n",
    "\n",
    "    Whether bootstrap samples are used when building trees.\n",
    "* **oob_score**: boolean, default=False\n",
    "  \n",
    "    Whether to use out-of-bag samples to estimate the generalization accuracy. Only available if bootstrap=True.\n",
    "* **warm_start**: boolean, default=False\n",
    "  \n",
    "    When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new ensemble.\n",
    "* **class_weight**: dict, list of dicts, “balanced”, or None, default=None\n",
    "  \n",
    "    Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y.\n",
    "    - For multi-output, the weights are assigned to the first output of each base learner.\n",
    "    - If “balanced”, then classes are reweighted such that they sum to n_samples.\n",
    "    - If a dict or list of dicts, then the weights are assigned to the keys.\n",
    "    - If None, then all classes are supposed to have weight one.\n",
    "* **max_sample_count**: int, default=None\n",
    "  \n",
    "    If bootstrap is True, the number of samples to draw from X to train each base estimator.\n",
    "\n",
    "    - If None (default), then draw X.shape[0] samples.\n",
    "\n",
    "    - If int, then draw max_samples samples.\n",
    "\n",
    "   - If float, then draw max_samples * X.shape[0] samples. Thus, max_samples should be in the interval (0.0, 1.0]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of the `RandomForestClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the attributes of the `RandomForestClassifier`:\n",
    "* **n_estimators_**: int\n",
    "  \n",
    "    The number of trees in the forest.\n",
    "* **estimators_**: list of estimators\n",
    "    \n",
    "    The collection of fitted base estimators.\n",
    "* **classes_**: ndarray of shape (n_classes,)\n",
    "      \n",
    "     The classes labels.\n",
    "* **base_estimator_**: estimator\n",
    "    \n",
    "    The base estimator from which the ensemble is grown.\n",
    "* **oob_score_**: float\n",
    "    \n",
    "    Score of the training dataset obtained using an out-of-bag estimate.\n",
    "* **feature_importances_**: ndarray of shape (n_features,)\n",
    "    \n",
    "    The feature importances attribute of the forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `RandomForestRegressor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of the `RandomForestRegressor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as `RandomForestClassifier` but with following exception/addition:\n",
    "* **criterion**: {“squared_error”, “absolute_error”, “poisson”}, default=”squared_error”\n",
    "  \n",
    "    The function to measure the quality of a split. Supported criteria are “squared_error” for the mean squared error, which is equal to variance reduction as feature selection criterion, “absolute_error” for the mean absolute error, and “poisson” which uses reduction in Poisson deviance to find splits. Training using “absolute_error” is significantly slower than when using “squared_error”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of the `RandomForestRegressor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as `RandomForestClassifier` excluding the class related attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ExtraTreesClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of the `ExtraTreesClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as `RandomForestClassifier`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of the `ExtraTreesClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as `RandomForestClassifier`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ExtraTreesRegressor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of the `ExtraTreesRegressor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as `RandomForestRegressor`. But the `criterion` parameter does not support “poisson”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of the `ExtraTreesRegressor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as `RandomForestRegressor`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `VotingClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of the `VotingClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the parameters of the `VotingClassifier`:\n",
    "* **voting**: {“hard”, “soft”}, default=”hard”\n",
    "  \n",
    "    If “hard”, uses predicted class labels for majority voting.\n",
    "    If “soft”, predicts the class label that is most common in the majority class subset.\n",
    "* **estimators**: tuples or list of estimator\n",
    "  \n",
    "    The base estimators for the voting classifier. If list of tuples, the weights of the estimators are used for voting. If list of estimators, the estimators are used for voting.\n",
    "* **weights**: array-like of shape (n_classifiers,), default=None\n",
    "  \n",
    "    Sequence of weights (float or int) to weight the occurrences of predicted class labels (hard voting) or class probabilities before averaging (soft voting). Uses uniform weights if None.\n",
    "* **n_jobs**: int, default=1\n",
    "    \n",
    "    The number of jobs to run in parallel for both `fit` and `predict`. If -1, then the number of jobs is set to the number of cores.\n",
    "* **flatten_transform**: boolean, default=True\n",
    "  \n",
    "    Affects shape of transform output only when voting=’soft’ If voting=’soft’ and flatten_transform=True, transform method returns matrix with shape (n_samples, n_classifiers * n_classes). If flatten_transform=False, it returns (n_classifiers, n_samples, n_classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of the `VotingClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the attributes of the `VotingClassifier`:\n",
    "* **estimators_**: list of estimators\n",
    "    \n",
    "    The collection of fitted base estimators.\n",
    "* **named_estimators**: dict of estimators\n",
    "        \n",
    "        A dictionary mapping the names of the estimators to the fitted estimators.\n",
    "* **le_**: LabelEncoder\n",
    "    \n",
    "    The LabelEncoder used to encode the class labels.\n",
    "* **classes_**: ndarray of shape (n_classes,)\n",
    "      \n",
    "     The classes labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `VotingRegressor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of the `VotingRegressor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the parameters of the `VotingRegressor`:\n",
    "* **estimators**: tuples or list of estimator\n",
    "  \n",
    "    The base estimators for the voting classifier. If list of tuples, the weights of the estimators are used for voting. If list of estimators, the estimators are used for voting.\n",
    "* **weights**: array-like of shape (n_classifiers,), default=None\n",
    "  \n",
    "    Sequence of weights (float or int) to weight the occurrences of predicted class labels (hard voting) or class probabilities before averaging (soft voting). Uses uniform weights if None.\n",
    "* **n_jobs**: int, default=1\n",
    "    \n",
    "    The number of jobs to run in parallel for both `fit` and `predict`. If -1, then the number of jobs is set to the number of cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of the `VotingRegressor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the attributes of the `VotingRegressor`:\n",
    "* **estimators_**: list of estimators\n",
    "    \n",
    "    The collection of fitted base estimators.\n",
    "* **named_estimators**: dict of estimators\n",
    "        \n",
    "        A dictionary mapping the names of the estimators to the fitted estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `StackingClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of the `StackingClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the parameters of the `StackingClassifier`:\n",
    "* **estimators**: list of estimators\n",
    "  \n",
    "    Base estimators which will be stacked together. Each element of the list is defined as a tuple of string (i.e. name) and an estimator instance. An estimator can be set to ‘drop’ using set_params.\n",
    "* **final_estimator**: estimator, default=None\n",
    "  \n",
    "    The final estimator to be used in the StackingClassifier.  The default classifier is a LogisticRegression.\n",
    "* **cv**: int, cross-validation generator or an iterable, default=None\n",
    "  \n",
    "    Determines the cross-validation splitting strategy. Possible inputs for cv are:\n",
    "    * None, to use the default 5-fold cross-validation,\n",
    "    * integer, to specify the number of folds.\n",
    "    * An object to be used as a cross-validation generator.\n",
    "    * An iterable yielding train, test splits.\n",
    "    * An iterable yielding (train, test) splits.\n",
    "    * A mapping from output names to (train, test) splits.\n",
    "    * A callable, which returns a training and test set.\n",
    "  \n",
    "    If “prefit” is passed, it is assumed that all estimators have been fitted already. The final_estimator_ is trained on the estimators predictions on the full training set and are not cross validated predictions. Please note that if the models have been trained on the same data to train the stacking model, there is a very high risk of overfitting.\n",
    "* **stack_method**: {‘auto’, ‘predict_proba’, ‘decision_function’, ‘predict’}, default=’auto’\n",
    "  \n",
    "    The method used to combine the predictions of the base estimators. If “predict”, the mean of the predicted values of the base estimators is used. If “predict_proba”, the mean of the predicted probabilities of the base estimators is used. If “decision_function”, the mean of the predicted decision function of the base estimators is used.\n",
    "* **passthrough**: boolean, default=False\n",
    "  \n",
    "    When False, only the predictions of estimators will be used as training data for final_estimator. When True, the final_estimator is trained on the predictions as well as the original training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of the `StackingClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the attributes of the `StackingClassifier`:\n",
    "* **estimators_**: list of estimators\n",
    "    \n",
    "    The collection of fitted base estimators.\n",
    "* **named_estimators**: dict of estimators\n",
    "        \n",
    "        A dictionary mapping the names of the estimators to the fitted estimators.\n",
    "* **classes_**: ndarray of shape (n_classes,)\n",
    "      \n",
    "     The classes labels.\n",
    "* **final_estimator_**: estimator\n",
    "  \n",
    "    TThe classifier which predicts given the output of `estimators_`.\n",
    "* **stack_method_**: list ot str\n",
    "  \n",
    "    The method used by each base estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `StackingRegressor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of the `StackingRegressor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as `StackingClassifier` except for the `stack_method` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of the `StackingRegressor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as `StackingClassifier` except for the class related attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `AdaBoostClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of the `AdaBoostClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the parameters of the `AdaBoostClassifier`:\n",
    "* **base_estimator**: estimator, default=None\n",
    "  \n",
    "    The base estimator to fit on random subsets of the dataset. If None, then the base estimator is a decision tree.\n",
    "* **n_estimators**: int, default=50\n",
    "  \n",
    "    The maximum number of estimators at which boosting is terminated.\n",
    "* **learning_rate**: float, default=1.\n",
    "  \n",
    "    Weight applied to each classifier at each boosting iteration. A higher learning rate increases the contribution of each classifier.of each classifier by `learning_rate`. There is a trade-off between `learning_rate` and `n_estimators`.\n",
    "* **algorithm**: {‘SAMME’, ‘SAMME.R’}, default=’SAMME.R’\n",
    "  \n",
    "    If ‘SAMME.R’ then use the SAMME.R real boosting algorithm. `base_estimator` must support calculation of class probabilities. If ‘SAMME’ then use the SAMME discrete boosting algorithm. The SAMME.R algorithm typically converges faster than SAMME, achieving a lower test error with fewer boosting iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of the `AdaBoostClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the attributes of the `AdaBoostClassifier`:\n",
    "* **estimators_**: list of estimators\n",
    "    \n",
    "    The collection of fitted base estimators.\n",
    "* **base_estimator_**: estimator\n",
    "  \n",
    "    The base estimator from which the boosted ensemble is built.\n",
    "* **classes_**: ndarray of shape (n_classes,)\n",
    "      \n",
    "     The classes labels.\n",
    "* **n_classes_**: int\n",
    "  \n",
    "    The number of classes.\n",
    "* **estimator_weights_**: ndarray of shape (n_estimators,)\n",
    "  \n",
    "    The weights for each estimator in the boosted ensemble.\n",
    "* **estimator_errors_**: ndarray of shape (n_estimators,)\n",
    "  \n",
    "    The classifier loss functions for each estimator in the boosted ensemble.\n",
    "* **feature_importances_**: ndarray of shape (n_features,)\n",
    "  \n",
    "  The impurity-based feature importances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `AdaBoostRegressor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of the `AdaBoostRegressor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as `AdaBoostClassifier` except for the `loss` parameter.\n",
    "* **loss**: {‘linear’, ‘square’, ‘exponential’}, default=’linear’\n",
    "  \n",
    "    The loss function to be used. ‘linear’, ‘square’, and ‘exponential’ perform linear, squared, and exponential loss, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of the `AdaBoostRegressor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as `AdaBoostClassifier` except for the class related attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `GradientBoostingClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the parameters of the `GradientBoostingClassifier`:\n",
    "* **loss**:{‘log_loss’, ‘deviance’, ‘exponential’}, default=’log_loss’\n",
    "  \n",
    "    The loss function to be optimized. ‘log_loss’ refers to binomial and multinomial deviance, the same as used in logistic regression. It is a good choice for classification with probabilistic outputs. For loss ‘exponential’, gradient boosting recovers the AdaBoost algorithm.\n",
    "* **learning_rate**: float, default=0.1\n",
    "  \n",
    "    Learning rate shrinks the contribution of each tree by `learning_rate`. There is a trade-off between `learning_rate` and `n_estimators`.\n",
    "* **n_estimators**: int, default=100\n",
    "      \n",
    "     The maximum number of estimators at which boosting is terminated.\n",
    "* **subsample**: float, default=1.0\n",
    "  \n",
    "    The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0, the model will randomly sample this fraction of samples when fitting each base learters. If greater than 1.0, it will use the entire dataset for fitting each base learters.\n",
    "* **criterion**: {‘friedman_mse’, ‘mse’, ‘mae’}, default=’friedman_mse’\n",
    "  \n",
    "    The function to measure the quality of a split. Supported criteria are ‘friedman_mse’, ‘mse’, and ‘mae’. The best split is the one that minimizes the criterion function.\n",
    "* **init**: estimator or ‘zero’, default=None\n",
    "  \n",
    "   An estimator object that is used to compute the initial predictions. `init` has to provide `fit` and `predict_proba`. If ‘zero’, the initial raw predictions are set to zero. By default, a `DummyEstimator` predicting the classes priors is used.\n",
    "* **validation_fraction**: float, default=0.1\n",
    "  \n",
    "    The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if validation_fraction > 0.\n",
    "\n",
    "**Plus other parameters of the `RandomForestClassifier` like `max_depth`, `min_samples_split`, `min_samples_leaf`, `min_weight_fraction_leaf`, `max_features`, `max_leaf_nodes`, `min_impurity_decrease`, `min_impurity_split`, `bootstrap`, `oob_score`, `n_jobs`, `random_state`, `verbose`, `warm_start`, `class_weight`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of the `GradientBoostingClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the attributes of the `GradientBoostingClassifier`:\n",
    "* **estimators_**: list of estimators\n",
    "    \n",
    "    The collection of fitted base estimators.\n",
    "* **n_estimators_**: int\n",
    "    \n",
    "    The number of estimators in the fitted ensemble.\n",
    "* **feature_importances_**: ndarray of shape (n_features,)\n",
    "    \n",
    "    The impurity-based feature importances.\n",
    "* **oob_improvement_**: ndarray of shape (n_estimators,)\n",
    "    \n",
    "    The improvement in loss (= deviance) on the out-of-bag samples relative to the previous iteration. `oob_improvement_[0]` is the improvement in loss of the first stage over the `init` estimator. Only available if `subsample < 1.0`\n",
    "* **train_score_**: ndarray of shape (n_estimators,)\n",
    "  \n",
    "    The i-th score train_score_[i] is the deviance (= loss) of the model at iteration i on the in-bag sample. If subsample == 1 this is the deviance on the training data.\n",
    "* **classes_**: ndarray of shape (n_classes,)\n",
    "      \n",
    "     The classes labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `GradientBoostingRegressor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of the `GradientBoostingRegressor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as `GradientBoostingClassifier` except for the `loss` parameter.\n",
    "* **loss**: ‘squared_error’, ‘absolute_error’, ‘huber’, ‘quantile’}, default=’squared_error’\n",
    "  \n",
    "    Loss function to be optimized. ‘squared_error’ refers to the squared error for regression. ‘absolute_error’ refers to the absolute error of regression and is a robust loss function. ‘huber’ is a combination of the two. ‘quantile’ allows quantile regression (use `alpha` to specify the quantile).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of the `GradientBoostingRegressor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as `GradientBoostingClassifier` except for the class related attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `HistGradientBoostingClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of the `HistGradientBoostingClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HHere are some of the parameters of the `HistGradientBoostingClassifier`:\n",
    "* **loss**: {‘log_loss’, ‘auto’, ‘binary_crossentropy’, ‘categorical_crossentropy’}, default=’log_loss’\n",
    "      \n",
    "     The loss function to use in the boosting process.\n",
    "\n",
    "    For binary classification problems, ‘log_loss’ is also known as logistic loss, binomial deviance or binary crossentropy. Internally, the model fits one tree per boosting iteration and uses the logistic sigmoid function (expit) as inverse link function to compute the predicted positive class probability.\n",
    "\n",
    "    For multiclass classification problems, ‘log_loss’ is also known as multinomial deviance or categorical crossentropy. Internally, the model fits one tree per boosting iteration and per class and uses the softmax function as inverse link function to compute the predicted probabilities of the classes.\n",
    "* **learning_rate**: float, default=0.1\n",
    "      \n",
    "    The learning rate, also known as shrinkage. This is used as a multiplicative factor for the leaves values. Use 1 for no shrinkage.\n",
    "* **max_iter**: int, default=100\n",
    "      \n",
    "    The maximum number of iterations of the boosting process, i.e. the maximum number of trees for binary classification. For multiclass classification, n_classes trees per iteration are built.\n",
    "* **l2_regularization**: float, default=0.0\n",
    "      \n",
    "    L2 regularization parameter. The larger this is, the more regularization, the less regularization, the more sparsity.\n",
    "* **monotonic_cst**: array-like of int of shape (n_features), default=Nonearray-like of int of shape (n_features), default=None\n",
    "      \n",
    "    Indicates the monotonic constraint to enforce on each feature. -1, 1 and 0 respectively correspond to a negative constraint, positive constraint and no constraint.\n",
    "* **early_stopping**: bool, default=False\n",
    "      \n",
    "    If ‘auto’, early stopping is enabled if the sample size is larger than 10000. If True, early stopping is enabled, otherwise early stopping is disabled.\n",
    "* **validation_fraction**: float, default=0.1\n",
    "  \n",
    "    The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if validation_fraction > 0.\n",
    "* **scoring**: str, default=None\n",
    "      \n",
    "    A string or a callable to evaluate the predictions on the test set. If ‘None’, the score method of the estimator is used. If a callable is given, it is called with two arguments and must return a scalar value.\n",
    "\n",
    "**Plus Other parameters from the `GradientBoostingClassifier` like `max_depth`, `min_samples_split`, `min_samples_leaf`, `min_weight_fraction_leaf`, `max_features`, `max_leaf_nodes`, `min_impurity_decrease`, `min_impurity_split`, `bootstrap`, `oob_score`, `n_jobs`, `random_state`, `verbose`, `warm_start`, `class_weight`**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of the `HistGradientBoostingClassifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the attributes of the `HistGradientBoostingClassifier`:\n",
    "* **classes_**: ndarray of shape (n_classes,)\n",
    "      \n",
    "     The classes labels.\n",
    "* **do_early_stopping_**: bool\n",
    "      \n",
    "     Whether early stopping is performed.\n",
    "* **n_iter_**: int\n",
    "      \n",
    "     The number of boosting iterations computed.\n",
    "* **train_score_**: ndarray of shape (n_iter,)\n",
    "    \n",
    "    The scores at each iteration on the training data. The first entry is the score of the ensemble before the first iteration. Scores are computed according to the scoring parameter. If `scoring` is not ‘loss’, scores are computed on a subset of at most 10 000 samples. Empty if no early stopping.\n",
    "* **validation_score_**: ndarray of shape (n_iter,)\n",
    "    \n",
    "    The scores at each iteration on the held-out validation data. The first entry is the score of the ensemble before the first iteration. Scores are computed according to the `scoring` parameter. Empty if no early stopping or if `validation_fraction` is None.\n",
    "* **n_trees_per_iteration_**: int\n",
    "    \n",
    "    The number of tree that are built at each iteration. This is equal to 1 for binary classification, and to `n_classes` for multiclass classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `HistGradientBoostingRegressor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of the `HistGradientBoostingRegressor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as `HistGradientBoostingClassifier` except for the `loss` parameter.\n",
    "* **loss**: {‘squared_error’, ‘absolute_error’, ‘poisson’, ‘quantile’}, default=’squared_error’\n",
    "  \n",
    "    The loss function to use in the boosting process. Note that the “squared error” and “poisson” losses actually implement “half least squares loss” and “half poisson deviance” to simplify the computation of the gradient. Furthermore, “poisson” loss internally uses a log-link and requires `y >= 0`. “quantile” uses the pinball loss.\n",
    "* **quantile**: float, default=None\n",
    "      \n",
    "    The quantile used to compute the quantile loss. Must be between 0 and 1. Only used if loss is ‘quantile’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of the `HistGradientBoostingRegressor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as `HistGradientBoostingClassifier` except for the class related attributes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98a06921e42c4ae689d578665f28201be7cad9b45c4149f8887161f1353af971"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
