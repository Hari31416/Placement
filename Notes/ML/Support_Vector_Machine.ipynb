{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"Contents\">Contents<a href=\"#Contents\"></a></h1>\n",
    "        <ol>\n",
    "        <li><a class=\"\" href=\"#Support-Vector-Machine\">Support Vector Machine</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Mathematical-Formulation\">Mathematical Formulation</a></li>\n",
    "<ol><li><a class=\"\" href=\"#SVC\">SVC</a></li>\n",
    "<li><a class=\"\" href=\"#LinearSVC\">LinearSVC</a></li>\n",
    "<li><a class=\"\" href=\"#SVR\">SVR</a></li>\n",
    "<li><a class=\"\" href=\"#LinearSVR\">LinearSVR</a></li>\n",
    "</ol><li><a class=\"\" href=\"#Classifier\">Classifier</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Multi-class-classification\">Multi-class classification</a></li>\n",
    "</ol><li><a class=\"\" href=\"#Regression\">Regression</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Complexity\">Complexity</a></li>\n",
    "</ol><li><a class=\"\" href=\"#Kernel\">Kernel</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Parameters-of-Kernel\">Parameters of Kernel</a></li>\n",
    "<li><a class=\"\" href=\"#Using-Python-functions-as-kernels\">Using Python functions as kernels</a></li>\n",
    "</ol><li><a class=\"\" href=\"#SVC\">SVC</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Parameters-of-SVC\">Parameters of SVC</a></li>\n",
    "<li><a class=\"\" href=\"#Attributes-of-SVC\">Attributes of SVC</a></li>\n",
    "</ol><li><a class=\"\" href=\"#SVR\">SVR</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Parameters-of-SVR\">Parameters of SVR</a></li>\n",
    "<li><a class=\"\" href=\"#Attributes-of-SVR\">Attributes of SVR</a></li>\n",
    "</ol><li><a class=\"\" href=\"#LinearSVC\">LinearSVC</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Attributes-of-LinearSVC\">Attributes of LinearSVC</a></li>\n",
    "</ol><li><a class=\"\" href=\"#LinearSVR\">LinearSVR</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Parameters-of-LinearSVR\">Parameters of LinearSVR</a></li>\n",
    "<li><a class=\"\" href=\"#Attributes-of-LinearSVR\">Attributes of LinearSVR</a></li>\n",
    "</ol><li><a class=\"\" href=\"#NuSVC\">NuSVC</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Parameters-of-NuSVC\">Parameters of NuSVC</a></li>\n",
    "<li><a class=\"\" href=\"#Attributes-of-NuSVC\">Attributes of NuSVC</a></li>\n",
    "</ol><li><a class=\"\" href=\"#NuSVR\">NuSVR</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Parameters-of-NuSVR\">Parameters of NuSVR</a></li>\n",
    "<li><a class=\"\" href=\"#Attributes-of-NuSVR\">Attributes of NuSVR</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n",
    "\n",
    "The advantages of support vector machines are:\n",
    "\n",
    "* Effective in high dimensional spaces.\n",
    "\n",
    "* Still effective in cases where number of dimensions is greater than the number of samples.\n",
    "\n",
    "* Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n",
    "\n",
    "* Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n",
    "\n",
    " The disadvantages of support vector machines include:\n",
    "\n",
    "* If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n",
    "\n",
    "* SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A support vector machine constructs a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class (so-called functional margin), since in general the larger the margin the lower the generalization error of the classifier. The figure below shows the decision function for a linearly separable problem, with three samples on the margin boundaries, called “support vectors”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_separating_hyperplane_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">In general, when the problem isn’t linearly separable, the support vectors are the samples within the margin boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given training vectors $x_i \\in \\mathbb{R}^p$ , i=1,…, n, in two classes, and a vector $y \\in \\{1, -1\\}^n$, our goal is to find $w \\in\n",
    "\\mathbb{R}^p$ and $b \\in \\mathbb{R}$ such that the prediction given by $\\text{sign} (w^T\\phi(x) + b)$ is correct for most samples.\n",
    "\n",
    "SVC solves the following primal problem:\n",
    "$$\n",
    "\\begin{align*}\\begin{aligned}\\min_ {w, b, \\zeta} \\frac{1}{2} w^T w + C \\sum_{i=1}^{n} \\zeta_i\\\\\\begin{split}\\textrm {subject to } & y_i (w^T \\phi (x_i) + b) \\geq 1 - \\zeta_i,\\\\\n",
    "& \\zeta_i \\geq 0, i=1, ..., n\\end{split}\\end{aligned}\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, we’re trying to maximize the margin (by minimizing $||w||^2 = w^Tw$), while incurring a penalty when a sample is misclassified or within the margin boundary. Ideally, the value $y_i\n",
    "(w^T \\phi (x_i) + b)$ would be $\\ge 1$ for all samples, which indicates a perfect prediction. But problems are usually not always perfectly separable with a hyperplane, so we allow some samples to be at a distance $\\zeta_i$ from their correct margin boundary. The penalty term `C` controls the strength of this penalty, and as a result, acts as an inverse regularization parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same problem can also be written as:\n",
    "$$\\begin{align*}\\begin{aligned}\\min_{\\alpha} \\frac{1}{2} \\alpha^T Q \\alpha - e^T \\alpha\\\\\\begin{split}\n",
    "\\textrm {subject to } & y^T \\alpha = 0\\\\\n",
    "& 0 \\leq \\alpha_i \\leq C, i=1, ..., n\\end{split}\\end{aligned}\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $e$ is the vector of all ones, and $Q$ is an $n$ by $n$ positive semidefinite matrix,$Q_{ij} \\equiv y_i y_j K(x_i, x_j)$ , where $K(x_i, x_j) = \\phi (x_i)^T \\phi (x_j)$ is the kernel. The terms $\\alpha_i$ are called the dual coefficients, and they are upper-bounded by C. This dual representation highlights the fact that training vectors are implicitly mapped into a higher (maybe infinite) dimensional space by the function $\\phi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the optimization problem is solved, the output of decision_function for a given sample $x$ becomes:\n",
    "$$\\sum_{i\\in SV} y_i \\alpha_i K(x_i, x) + b,$$\n",
    "and the predicted class correspond to its sign. We only need to sum over the support vectors (i.e. the samples that lie within the margin) because the dual coefficients $\\alpha_i$ are zero for the other samples.\n",
    "\n",
    "These parameters can be accessed through the attributes `dual_coef_` which holds the product $y_i \\alpha_i$, `support_vectors_` which holds the support vectors, and `intercept_` which holds the independent term b.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primal problem can be equivalently formulated as\n",
    "$$\\min_ {w, b} \\frac{1}{2} w^T w + C \\sum_{i=1}^{n}\\max(0, 1 - y_i (w^T \\phi(x_i) + b)),$$\n",
    "where we make use of the hinge loss. This is the form that is directly optimized by `LinearSVC`, but unlike the dual form, this one does not involve inner products between samples, so the famous kernel trick cannot be applied. This is why only the linear kernel is supported by `LinearSVC`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given training vectors $x_i \\in \\mathbb{R}^p$, i=1,…, n, and a vector $y \\in \\mathbb{R}^n$ $\\epsilon$-SVR solves the following primal problem:\n",
    "$$\n",
    "\\begin{align*}\\begin{aligned}\\min_ {w, b, \\zeta, \\zeta^*} \\frac{1}{2} w^T w + C \\sum_{i=1}^{n} (\\zeta_i + \\zeta_i^*)\\\\\\begin{split}\\textrm {subject to } & y_i - w^T \\phi (x_i) - b \\leq \\varepsilon + \\zeta_i,\\\\\n",
    "                      & w^T \\phi (x_i) + b - y_i \\leq \\varepsilon + \\zeta_i^*,\\\\\n",
    "                      & \\zeta_i, \\zeta_i^* \\geq 0, i=1, ..., n\\end{split}\\end{aligned}\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are penalizing samples whose prediction is at least $\\epsilon$ away from their true target. These samples penalize the objective by $\\zeta_i$ or $\\zeta_i^*$\n",
    ", depending on whether their predictions lie above or below the $\\epsilon$ tube."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dual problem is:\n",
    "$$\n",
    "\\begin{align*}\\begin{aligned}\\min_{\\alpha, \\alpha^*} \\frac{1}{2} (\\alpha - \\alpha^*)^T Q (\\alpha - \\alpha^*) + \\varepsilon e^T (\\alpha + \\alpha^*) - y^T (\\alpha - \\alpha^*)\\\\\\begin{split}\n",
    "\\textrm {subject to } & e^T (\\alpha - \\alpha^*) = 0\\\\\n",
    "& 0 \\leq \\alpha_i, \\alpha_i^* \\leq C, i=1, ..., n\\end{split}\\end{aligned}\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $e$ is the vector of all ones, $Q$ is an $n$ by $n$ positive semidefinite matrix, $Q_{ij} \\equiv K(x_i, x_j) = \\phi (x_i)^T \\phi (x_j)$ is the kernel. Here training vectors are implicitly mapped into a higher (maybe infinite) dimensional space by the function $\\phi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction is:\n",
    "$$\n",
    "\\sum_{i \\in SV}(\\alpha_i - \\alpha_i^*) K(x_i, x) + b\n",
    "$$\n",
    "These parameters can be accessed through the attributes `dual_coef_` which holds the difference $\\alpha_i - \\alpha_i^*$\n",
    ", `support_vectors_` which holds the support vectors, and `intercept_` which holds the independent term $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearSVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primal problem can equivalenty be written as:\n",
    "$$\\min_ {w, b} \\frac{1}{2} w^T w + C \\sum_{i=1}\\max(0, |y_i - (w^T \\phi(x_i) + b)| - \\varepsilon),$$\n",
    "where we make use of the epsilon-insensitive loss, i.e. errors of less than $\\epsilon$ are ignored. This is the form that is directly optimized by `LinearSVR`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SVC`, `NuSVC` and `LinearSVC` are classes capable of performing binary and multi-class classification on a dataset. `SVC` and `NuSVC` are similar methods, but accept slightly different sets of parameters and have different mathematical formulations (see section Mathematical formulation). On the other hand, `LinearSVC` is another (faster) implementation of Support Vector Classification for the case of a linear kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SVC` and `NuSVC` implement the “one-versus-one” approach for multi-class classification. In total, n_classes * (n_classes - 1) / 2 classifiers are constructed and each one trains data from two classes. To provide a consistent interface with other classifiers, the decision_function_shape option allows to monotonically transform the results of the “one-versus-one” classifiers to a “one-vs-rest” decision function of shape (n_samples, n_classes).\n",
    "\n",
    "On the other hand, `LinearSVC` implements “one-vs-the-rest” multi-class strategy, thus training n_classes models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method of Support Vector Classification can be extended to solve regression problems. This method is called Support Vector Regression.\n",
    "\n",
    "The model produced by support vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by Support Vector Regression depends only on a subset of the training data, because the cost function ignores samples whose prediction is close to their target.\n",
    "\n",
    "There are three different implementations of Support Vector Regression: `SVR`, `NuSVR` and `LinearSVR`. `LinearSVR` provides a faster implementation than `SVR` but only considers the linear kernel, while `NuSVR` implements a slightly different formulation than `SVR` and `LinearSVR`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines are powerful tools, but their compute and storage requirements increase rapidly with the number of training vectors. The core of an SVM is a quadratic programming problem (QP), separating support vectors from the rest of the training data. The QP solver used by the libsvm-based implementation scales between $O(n_{features} \\times n_{samples}^2)$\n",
    " and $O(n_{features} \\times n_{samples}^3)$\n",
    " depending on how efficiently the libsvm cache is used in practice (dataset dependent). If the data is very sparse  should be replaced by the average number of non-zero features in a sample vector.\n",
    "\n",
    "For the linear case, the algorithm used in `LinearSVC` by the liblinear implementation is much more efficient than its libsvm-based `SVC` counterpart and can scale almost linearly to millions of samples and/or features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernels are used to transform the training vectors into a higher dimensional space. The kernel function is used to compute the dot product between two vectors. Here are some of the kernels implemented in scikit-learn:\n",
    "* Linear: $K(x, y) = x^T y$\n",
    "* Polynomial: $K(x, y) = (\\gamma x^T y + r)^{d}$\n",
    "* rbf: $K(x, y) = exp(-\\gamma \\|x - y\\|^2)$\n",
    "* sigmoid: $K(x, y) = \\frac{1}{1 + exp(-\\gamma x^T y +r)}$\n",
    " \n",
    "Here, d is specified by the `degree` parameter, $\\gamma$ is specified by the `gamma` parameter, and $r$ is specified by the `coef0` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter `C`, common to all SVM kernels, trades off misclassification of training examples against simplicity of the decision surface. A low `C` makes the decision surface smooth, while a high `C` aims at classifying all training examples correctly. `gamma` defines how much influence a single training example has. The larger `gamma` is, the closer other examples must be to be affected.\n",
    "\n",
    "Proper choice of `C` and `gamma` is critical to the SVM’s performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Python functions as kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use your own defined kernels by passing a function to the kernel parameter.\n",
    "\n",
    "Your kernel must take as arguments two matrices of shape `(n_samples_1, n_features)`, `(n_samples_2, n_features)` and return a kernel matrix of shape `(n_samples_1, n_samples_2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "def my_kernel(X, Y):\n",
    "    return np.dot(X, Y.T)\n",
    "\n",
    "clf = svm.SVC(kernel=my_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the parameters of the `SVC` class:\n",
    "* **C**: float, optional (default=1.0)\n",
    "    \n",
    "    Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.\n",
    "* **kernel**: {‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’} or callable, (default='rbf')\n",
    "    \n",
    "    Specifies the kernel type to be used in the algorithm. It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable. If none is given, ‘rbf’ will be used. If a callable is given it is used to precompute the kernel matrix.\n",
    "* **degree**: int, optional (default=3)\n",
    "  \n",
    "    Degree of the polynomial kernel function (‘poly’). Ignored by all other kernels.\n",
    "* **gamma**: float, optional (default=’auto’)\n",
    "    \n",
    "    Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’. \n",
    "    * if gamma='scale' (default) is passed then it uses 1 / (n_features * X.var()) as value of gamma,\n",
    "\n",
    "    * if ‘auto’, uses 1 / n_features.\n",
    "* **coef0**: float, optional (default=0.0)\n",
    "    \n",
    "    Independent term in kernel function. It is only significant in ‘poly’ and ‘sigmoid’.\n",
    "* **probability**: boolean, optional (default=False)\n",
    "  \n",
    "  Whether to enable probability estimates. This must be enabled prior to calling fit, will slow down that method as it internally uses 5-fold cross-validation, and predict_proba may be inconsistent with predict.\n",
    "* **tol**: float, optional (default=1e-3)\n",
    "    \n",
    "    Tolerance for stopping criterion.\n",
    "* **cache_size**: float, optional (default=200.0)\n",
    "\n",
    "    Specify the size of the kernel cache (in MB). \n",
    "* **class_weight**: {dict, ‘balanced’}, optional (default=None)\n",
    "    \n",
    "    Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all classes are supposed to have weight one. The ‘balanced’ mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as `n_samples / (n_classes * np.bincount(y))`  \n",
    "* **verbose**: int, optional (default=0)\n",
    "  \n",
    "    Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context.\n",
    "* **max_iter**: int, optional (default=-1)\n",
    "  \n",
    "    Hard limit on iterations within solver, or -1 for no limit.\n",
    "* **decision_function_shape**: [‘ovo’, ‘ovr’], optional (default=’ovr’)\n",
    "    \n",
    "    Whether to return a one-vs-rest (‘ovr’) decision function of shape (n_samples, n_classes) or one-vs-one (‘ovo’) decision function of shape (n_samples, n_classes * (n_classes - 1) / 2).\n",
    "\n",
    "    However, note that internally, one-vs-one (‘ovo’) is always used as a multi-class strategy to train models; an ovr matrix is only constructed from the ovo matrix. The parameter is ignored for binary classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the attributes of the `SVC` class:\n",
    "* **support_**: array-like, shape = [n_SV, n_features]\n",
    "    \n",
    "    Support vectors.\n",
    "* **support_vectors_**: array-like, shape = [n_SV, n_features]\n",
    "  \n",
    "    Support vectors.\n",
    "* **class_weight_**: array-like, shape = [n_classes]\n",
    "    \n",
    "    Multipliers of parameter C for each class. Computed based on the `class_weight` parameter.\n",
    "* **classes_**: array-like, shape = [n_classes]\n",
    "    \n",
    "    The classes labels.\n",
    "* **coef_**: ndarray of shape (n_classes * (n_classes - 1) / 2, n_features)\n",
    "    \n",
    "    Weights assigned to the features when `kernel=\"linear\"`.\n",
    "\n",
    "* **dual_coef_**: ndarray of shape (n_classes -1, n_SV)\n",
    "    \n",
    "    Coefficients of the support vectors in the decision function.\n",
    "* **intercept_**: ndarray of shape (n_classes * (n_classes - 1) / 2,)\n",
    "    \n",
    "    Constants in decision function.\n",
    "* **probA_**: ndarray of shape (n_classes * (n_classes - 1) / 2)\n",
    "    \n",
    "    Parameter learned in Platt scaling when probability=True."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SVR` class has the same parameters as the `SVC` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SVR` class has the same attributes as the `SVC` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some of the parameters of the `LinearSVC` class:\n",
    "* **penalty**: {‘l1’, ‘l2’}, optional (default=’l2’)\n",
    "    \n",
    "    Specifies the norm used in the penalization. The ‘l2’ penalty is the standard used in SVC. The ‘l1’ leads to coef_ vectors that are sparse.\n",
    "* **loss**: {‘hinge’, ‘squared_hinge’}, optional (default=’squared_hinge’)\n",
    "  \n",
    "    Specifies the loss function. ‘hinge’ is the standard SVM loss (used e.g. by the SVC class) while ‘squared_hinge’ is the square of the hinge loss. The combination of `penalty='l1'` and `loss='hinge'` is not supported.\n",
    "* **dual**: boolean, optional (default=True)\n",
    "        \n",
    "        Whether to fit the classifier dual or primal. Prefer dual=False when n_samples > n_features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other parameters are the same as for the `SVC` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as for the `SVC` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearSVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of LinearSVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LinearSVR` and the `SVR` clases have a lot of parameters in common. There are, however, some differences:\n",
    "* **epsilon**: float, optional (default=0.0)\n",
    "    \n",
    "    Epsilon parameter in the epsilon-insensitive loss function.\n",
    "* **loss**: {‘epsilon_insensitive’, ‘squared_epsilon_insensitive’}, optional (default=’epsilon_insensitive’)\n",
    "  \n",
    "    Specifies the loss function. The epsilon-insensitive loss (standard SVR) is the L1 loss, while the squared epsilon-insensitive loss (‘squared_epsilon_insensitive’) is the L2 loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of LinearSVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as for the `SVR` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NuSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of NuSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as SVC with some differences as:\n",
    "* **nu**: float, optional (default=0.5)\n",
    "    \n",
    "   An upper bound on the fraction of margin errors and a lower bound of the fraction of support vectors. Should be in the interval (0, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of NuSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as for the `SVC` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NuSVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters of NuSVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as `NuSVC`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of NuSVR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as for the `NuSVC` class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "21040c1b576dca9f4f330277849b9f4819256d524dee23c2b89e431027dafe11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
