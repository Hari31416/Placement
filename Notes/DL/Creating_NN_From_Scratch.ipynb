{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Contents\">Contents<a href=\"#Contents\"></a></h2>\n",
    "        <ol>\n",
    "        <li><a class=\"\" href=\"#Imports\">Imports</a></li>\n",
    "<li><a class=\"\" href=\"#Notation\">Notation</a></li>\n",
    "<li><a class=\"\" href=\"#Outline\">Outline</a></li>\n",
    "<li><a class=\"\" href=\"#Forward-Propagation\">Forward Propagation</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Initialization\">Initialization</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Getting-the-Shapes-Right\">Getting the Shapes Right</a></li>\n",
    "<li><a class=\"\" href=\"#initialize_parameters_deep\">initialize_parameters_deep</a></li>\n",
    "</ol><li><a class=\"\" href=\"#Activation-Functions\">Activation Functions</a></li>\n",
    "<ol><li><a class=\"\" href=\"#relu\">relu</a></li>\n",
    "<li><a class=\"\" href=\"#sigmoid\">sigmoid</a></li>\n",
    "</ol><li><a class=\"\" href=\"#Stacking-Linear-Layer-with-Activation\">Stacking Linear Layer with Activation</a></li>\n",
    "<ol><li><a class=\"\" href=\"#linear_forward\">linear_forward</a></li>\n",
    "<li><a class=\"\" href=\"#linear_activation_forward\">linear_activation_forward</a></li>\n",
    "<li><a class=\"\" href=\"#L_model_forward\">L_model_forward</a></li>\n",
    "</ol></ol><li><a class=\"\" href=\"#The-Cost\">The Cost</a></li>\n",
    "<li><a class=\"\" href=\"#Backpropagation\">Backpropagation</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Linear-Backward\">Linear Backward</a></li>\n",
    "<ol><li><a class=\"\" href=\"#linear_backward\">linear_backward</a></li>\n",
    "</ol><li><a class=\"\" href=\"#Backpropagation-for-Activation-Functions\">Backpropagation for Activation Functions</a></li>\n",
    "<ol><li><a class=\"\" href=\"#relu_backward\">relu_backward</a></li>\n",
    "<li><a class=\"\" href=\"#sigmoid_backward\">sigmoid_backward</a></li>\n",
    "</ol><li><a class=\"\" href=\"#Linear-Activation-Backward\">Linear-Activation Backward</a></li>\n",
    "<ol><li><a class=\"\" href=\"#linear_activation_backward\">linear_activation_backward</a></li>\n",
    "</ol><li><a class=\"\" href=\"#L-Model-Backward\">L-Model Backward</a></li>\n",
    "<ol><li><a class=\"\" href=\"#L_model_backward\">L_model_backward</a></li>\n",
    "</ol><li><a class=\"\" href=\"#Update-Parameters\">Update Parameters</a></li>\n",
    "<ol><li><a class=\"\" href=\"#update_parameters\">update_parameters</a></li>\n",
    "</ol><li><a class=\"\" href=\"#The-Model\">The Model</a></li>\n",
    "<ol><li><a class=\"\" href=\"#L_layer_model\">L_layer_model</a></li>\n",
    "<li><a class=\"\" href=\"#Train-the-model\">Train the model</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Creating-the-Dataset\">Creating the Dataset</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "plt.rcdefaults()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the notation we'll using in this notebook:\n",
    "- Superscript $[l]$ denotes a quantity associated with the $l^{th}$ layer. \n",
    "    - Example: $a^{[L]}$ is the $L^{th}$ layer activation. $W^{[L]}$ and $b^{[L]}$ are the $L^{th}$ layer parameters.\n",
    "- Superscript $(i)$ denotes a quantity associated with the $i^{th}$ example. \n",
    "    - Example: $x^{(i)}$ is the $i^{th}$ training example.\n",
    "- Lowerscript $i$ denotes the $i^{th}$ entry of a vector.\n",
    "    - Example: $a^{[l]}_i$ denotes the $i^{th}$ entry of the $l^{th}$ layer's activations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to create some helper functions which will be used later to create an arbitrarily deep neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an outline of the steps in this assignment:\n",
    "\n",
    "- Initialize the parameters for an $L$-layer neural network\n",
    "- Implement the forward propagation module (shown in purple in the figure below)\n",
    "     - Complete the LINEAR part of a layer's forward propagation step (resulting in $Z^{[l]}$).\n",
    "     - Implement the activation functions (relu/sigmoid)\n",
    "     - Combine the previous two steps into a new [LINEAR->ACTIVATION] forward function.\n",
    "     - Stack the [LINEAR->RELU] forward function L-1 time (for layers 1 through L-1) and add a [LINEAR->SIGMOID] at the end (for the final layer $L$). This gives you a new `L_model_forward` function.\n",
    "- Compute the loss\n",
    "- Implement the backward propagation module (denoted in red in the figure below)\n",
    "    - Complete the LINEAR part of a layer's backward propagation step\n",
    "    - Create functions for the gradient of the ACTIVATE function(relu_backward/sigmoid_backward) \n",
    "    - Combine the previous two steps into a new [LINEAR->ACTIVATION] backward function\n",
    "    - Stack [LINEAR->RELU] backward L-1 times and add [LINEAR->SIGMOID] backward in a new `L_model_backward` function\n",
    "- Finally, update the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/0201.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><center><b>Outline</b></center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every forward function, there is a corresponding backward function. This is why at every step of your forward module you will be storing some values in a cache. These cached values are useful for computing gradients.\n",
    "\n",
    "In the backpropagation module, you can then use the cache to calculate the gradients. Don't worry, this assignment will show you exactly how to carry out each of these steps!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Shapes Right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we create a function to initialize the parameters, we should make sure that the weights are of the correct shape. Assuming  the size of your input $X$ is $(12288, 209)$ (with $m=209$ examples) then the shapes of weights and biases of various layers should be:\n",
    "<table style=\"width:100%\">\n",
    "    <tr>\n",
    "        <td>  </td> \n",
    "        <td> <b>Shape of W</b> </td> \n",
    "        <td> <b>Shape of b</b>  </td> \n",
    "        <td> <b>Activation</b> </td>\n",
    "        <td> <b>Shape of Activation</b> </td> \n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td> <b>Layer 1</b> </td> \n",
    "        <td> $(n^{[1]},12288)$ </td> \n",
    "        <td> $(n^{[1]},1)$ </td> \n",
    "        <td> $Z^{[1]} = W^{[1]}  X + b^{[1]} $ </td> \n",
    "        <td> $(n^{[1]},209)$ </td> \n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td> <b>Layer 2</b> </td> \n",
    "        <td> $(n^{[2]}, n^{[1]})$  </td> \n",
    "        <td> $(n^{[2]},1)$ </td> \n",
    "        <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ </td> \n",
    "        <td> $(n^{[2]}, 209)$ </td> \n",
    "    <tr>\n",
    "       <tr>\n",
    "        <td> $\\vdots$ </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$</td> \n",
    "        <td> $\\vdots$  </td> \n",
    "    <tr>  \n",
    "   <tr>\n",
    "       <td> <b>Layer L-1</b> </td> \n",
    "        <td> $(n^{[L-1]}, n^{[L-2]})$ </td> \n",
    "        <td> $(n^{[L-1]}, 1)$  </td> \n",
    "        <td>$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ </td> \n",
    "        <td> $(n^{[L-1]}, 209)$ </td> \n",
    "   <tr>\n",
    "   <tr>\n",
    "       <td> <b>Layer L</b> </td> \n",
    "        <td> $(n^{[L]}, n^{[L-1]})$ </td> \n",
    "        <td> $(n^{[L]}, 1)$ </td>\n",
    "        <td> $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>\n",
    "        <td> $(n^{[L]}, 209)$  </td> \n",
    "    <tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping this table in mind, let's initialize the parameters for for an L-layer Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `initialize_parameters_deep`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps**\n",
    "\n",
    "- The model's structure is *[LINEAR -> RELU] $ \\times$ (L-1) -> LINEAR -> SIGMOID*. i.e., it has $L-1$ layers using a ReLU activation function followed by an output layer with a sigmoid activation function.\n",
    "- Use random initialization for the weight matrices. Use `np.random.randn(shape) * 0.01`.\n",
    "- Use zeros initialization for the biases. Use `np.zeros(shape)`.\n",
    "- We'll store $n^{[l]}$, the number of units in different layers, in a variable `layer_dims`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    layer_dims : list of ints\n",
    "        The dimensions of each layer in the network.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    parameters : dictionary\n",
    "        A dictionary containing the parameters of the network.\\\\\n",
    "        The dictionary has the following keys:\n",
    "        * Wl: The weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "        * bl: The bias vector of shape (layer_dims[l], 1)\\\\\n",
    "        for l in range(1, len(layer_dims))\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll implement the two activation functions that you'll use in the model:\n",
    "1. **Relu:** $\\max(0, x)$\n",
    "2. **Sigmod:** $\\frac{1}{1+e^{-x}}$\n",
    "The functions will also return the value of `x` as cache to be used in backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `relu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    x : numpy array\n",
    "        The input array.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    x : numpy array\n",
    "        The output array.\n",
    "    \"\"\"\n",
    "    return np.maximum(0, x), x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.linspace(-1, 1, 100)\n",
    "Y, _ = relu(X)\n",
    "plt.plot(X, Y)\n",
    "plt.axvline(x=0, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sigmoid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    x : numpy array\n",
    "        The input array.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    x : numpy array\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x)), x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.linspace(-10, 10, 200)\n",
    "Y, _ = sigmoid(X)\n",
    "plt.plot(X, Y)\n",
    "plt.axvline(x=0, color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking Linear Layer with Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll be stacking a linear layer with a ReLU activation layer to create one layer of the neural network. We'll also be returning the values of `A`, `W` and `b` as `cache` to be used later during backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of a unit without any activation function is given by:\n",
    "$$Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$$\n",
    "If $g$ is the activation function, then the activation of a unit is:\n",
    "$$A^{[l]} = g(Z^{[l]})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `linear_forward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "    Attributes\n",
    "    ----------\n",
    "    A : numpy array\n",
    "        The input array.\n",
    "    W : numpy array\n",
    "        The weight matrix.\n",
    "    b : numpy array\n",
    "        The bias vector.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Z : numpy array\n",
    "        The output array.\n",
    "    cache : tuple\n",
    "        A tuple of (A, W, b) to be used in the backward pass.\n",
    "    \"\"\"\n",
    "    Z = np.dot(W, A) + b\n",
    "    assert (Z.shape == (W.shape[0], A.shape[1])), \"Shape of Z is not correct\"\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `linear_activation_forward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    A_prev : numpy array\n",
    "        The input array.\n",
    "    W : numpy array\n",
    "        The weight matrix.\n",
    "    b : numpy array\n",
    "        The bias vector.\n",
    "    activation : string\n",
    "        The activation function to use. Available functions are:\n",
    "        * relu\n",
    "        * sigmoid\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A : numpy array\n",
    "        The output array.\n",
    "    cache : tuple\n",
    "        a python tuple containing \"linear_cache\" and \"activation_cache\"\\\\\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `L_model_forward`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now as we have the `linear_activation_forward` function implemented, we can implement the `L_model_forward` function. The function loops through `linear_activation_forward` to  LINEAR->RELU (L-1) times. Also, the function adds the output from the activation function to the cache. After that, the function also implments the LINEAR->SIGMOID as the final layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    X : numpy array\n",
    "        The input data, of shape (input size, number of examples)\n",
    "    parameters : dictionary\n",
    "        A dictionary containing the parameters of the network.\\\\\n",
    "        The dictionary has the following keys:\n",
    "        * Wl: The weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "        * bl: The bias vector of shape (layer_dims[l], 1)\\\\\n",
    "        for l in range(1, len(layer_dims))\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    AL : numpy array\n",
    "        The output of the forward propagation, of shape (output size, number of examples)\n",
    "    caches : list\n",
    "        A list of tuples, where each tuple contains the inputs and outputs of a layer.\\\\\n",
    "        For each layer, the tuple contains (A_prev, W, b)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    # number of layers in the neural network\n",
    "    L = len(parameters) // 2                  \n",
    "    \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters[f'W{l}'], parameters[f'b{l}'], 'relu')\n",
    "        caches.append(cache)\n",
    "    AL, cache = linear_activation_forward(A, parameters[f'W{L}'], parameters[f'b{L}'], 'sigmoid')\n",
    "    caches.append(cache)\n",
    "          \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll implement the cost function for the model. We'll use the cross-entropy cost function $J$, using the following formula: \n",
    "$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    AL : numpy array\n",
    "        The output of the forward propagation, of shape (1, number of examples)\n",
    "    Y : numpy array\n",
    "        The true labels, of shape (1, number of examples)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    cost = -(np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL)))/m\n",
    "    cost = np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will be about implementing the backpropagation algorithm to compute the gradients.\n",
    "![](images/0202.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "similarly to forward propagation, you're going to build the backward propagation in three steps:\n",
    "1. LINEAR backward\n",
    "2. LINEAR -> ACTIVATION backward where ACTIVATION computes the derivative of either the ReLU or sigmoid activation\n",
    "3. [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID backward (whole model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For layer $l$, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (followed by an activation).\n",
    "\n",
    "Suppose you have already calculated the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. You want to get $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ are computed using the input $dZ^{[l]}$.\n",
    "\n",
    "Here are the formulas:\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} $$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{J} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `linear_backward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_backward\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    dZ : numpy array\n",
    "        The gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache : tuple\n",
    "        A tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dA_prev : numpy array\n",
    "        The gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW : numpy array\n",
    "        The gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db : numpy array\n",
    "        The gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    dW = np.dot(dZ, A_prev.T)/m\n",
    "    db = np.sum(dZ,axis = 1, keepdims=True)/m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation for Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create two functions to implement the backward propagation for the activation functions, `relu_backward` and `sigmoid_backward`. The function will `dA` as well as `cache` as inputs and return `dZ`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `relu_backward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    dA : numpy array\n",
    "        post-activation gradient\n",
    "    cache : tuple\n",
    "        tuple of values (Z) from forward propagation\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dZ : numpy array\n",
    "        Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sigmoid_backward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    dA : numpy array\n",
    "        post-activation gradient\n",
    "    cache : tuple\n",
    "        tuple of values (Z) from forward propagation\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dZ : numpy array\n",
    "        Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Linear-Activation Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will create a function that merges the two helper functions: `linear_backward` and the backward step for the activation `linear_activation_backward`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `linear_activation_backward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    dA : numpy array\n",
    "        post-activation gradient for current layer l\n",
    "    cache : tuple\n",
    "        tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation : string\n",
    "        the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dA_prev : numpy array\n",
    "        Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW : numpy array\n",
    "        Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db : numpy array\n",
    "        Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)     \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  L-Model Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we implemented the L_model_forward function, at each iteration, we stored a cache which contains (X,W,b, and z). In the back propagation module, we'll use those variables to compute the gradients. Therefore, in the L_model_backward function, we'll iterate through all the hidden layers backward, starting from layer  L . On each step, we will use the cached values for layer  l  to backpropagate through layer l ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first, we need an initial value of `dA`, which is the derivative of the cost with respect to the activation of the output layer. This can be calculated using the formula:\n",
    "$$\n",
    "dA^{[L]} = -\\left(\\frac{Y}{A_L}-\\frac{1-Y}{1-A_L}\\right)\n",
    "$$\n",
    "We can then use this post-activation gradient dAL to keep going backward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `L_model_backward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    AL : numpy array\n",
    "        probability vector, output of forward propagation (L_model_forward())\n",
    "    Y : numpy array\n",
    "        true \"label\" vector\n",
    "    caches : list\n",
    "        list of caches containing:\n",
    "            every cache of linear_activation_forward() with \"relu\" (there are (L-1) ors)\n",
    "            the cache of linear_activation_forward() with \"sigmoid\" (there is one)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    grads : dictionary\n",
    "\n",
    "        grads[\"dA\" + str(l)] = ...\\\\\n",
    "        grads[\"dW\" + str(l)] = ...\\\\\n",
    "        grads[\"db\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, 'sigmoid')\n",
    "    grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
    "    grads[\"dW\" + str(L)] = dW_temp\n",
    "    grads[\"db\" + str(L)] = db_temp\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dA_prev_temp, current_cache, 'relu')\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll update the parameters of the model, using gradient descent: \n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `update_parameters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    params : dictionary\n",
    "        parameters to be updated\n",
    "    grads : dictionary\n",
    "        python dictionary containing your gradients, output of L_model_backward\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    params : dictionary\n",
    "        python dictionary containing your updated parameters after one pass of gradient descent\n",
    "            parameters[\"W\" + str(l)] = ... \\\\\n",
    "            parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    parameters = params.copy()\n",
    "    grads = grads.copy()\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)]- learning_rate*grads[\"db\" + str(l+1)]\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes the implementation of the helper functions needed to build a neural network. Next, we'll use those functions to build the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll build a L-layer neural network with the following structure: *[LINEAR -> RELU]  (L-1) -> LINEAR -> SIGMOID*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of layers and the number of units in each layer will be determined using the `layers_dims` parameter which is a list containing the number of units in each layer of the network. Let's create the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `L_layer_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_layer_model\n",
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    X : numpy array\n",
    "        (n_x, m) matrix, training examples\n",
    "    Y : numpy array\n",
    "        (1, m) matrix, training labels\n",
    "    layers_dims : list\n",
    "        dimensions of each layer\n",
    "    learning_rate : float\n",
    "        learning rate of the gradient descent update rule\n",
    "    num_iterations : int\n",
    "        number of iterations of the optimization loop\n",
    "    print_cost : bool\n",
    "        if True, it prints the cost every 100 steps\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    parameters : dictionary\n",
    "        parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    for i in range(0, num_iterations):\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n",
    "            print(f\"Cost after iteration {i}: {np.squeeze(cost)}\")\n",
    "        if i % 100 == 0 or i == num_iterations:\n",
    "            costs.append(cost)\n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dataset to be trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (12288, 209)\n",
      "test_x's shape: (12288, 50)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples \n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [12288, 20, 7, 5, 1] #  4-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6931489045172448\n",
      "Cost after iteration 100: 0.6624971419117535\n",
      "Cost after iteration 200: 0.651094757435918\n",
      "Cost after iteration 300: 0.6467572191732891\n",
      "Cost after iteration 400: 0.6450746296904617\n",
      "Cost after iteration 500: 0.6444126114559184\n",
      "Cost after iteration 600: 0.6441496227067939\n",
      "Cost after iteration 700: 0.6440444862712489\n",
      "Cost after iteration 800: 0.6440022822578234\n",
      "Cost after iteration 900: 0.6439852956688383\n",
      "Cost after iteration 1000: 0.643978446974854\n",
      "Cost after iteration 1100: 0.6439756825182593\n",
      "Cost after iteration 1200: 0.643974565630174\n",
      "Cost after iteration 1300: 0.6439741139342244\n",
      "Cost after iteration 1400: 0.6439739310230211\n",
      "Cost after iteration 1500: 0.6439738567187566\n",
      "Cost after iteration 1600: 0.643973826351625\n",
      "Cost after iteration 1700: 0.643973813766312\n",
      "Cost after iteration 1800: 0.6439738083386956\n",
      "Cost after iteration 1900: 0.6439738057896578\n",
      "Cost after iteration 2000: 0.6439738043154932\n",
      "Cost after iteration 2100: 0.6439738031521817\n",
      "Cost after iteration 2200: 0.643973802338866\n",
      "Cost after iteration 2300: 0.6439738016972762\n",
      "Cost after iteration 2400: 0.6439738011387375\n",
      "Cost after iteration 2499: 0.6439738006062699\n"
     ]
    }
   ],
   "source": [
    "parameters, costs = L_layer_model(train_x, train_y, layers_dims, learning_rate=0.02, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6555023923444976\n"
     ]
    }
   ],
   "source": [
    "pred_train = predict(train_x, train_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3400000000000001\n"
     ]
    }
   ],
   "source": [
    "pred_test = predict(test_x, test_y, parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
