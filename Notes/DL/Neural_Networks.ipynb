{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Contents\">Contents<a href=\"#Contents\"></a></h2>\n",
    "        <ol>\n",
    "        <li><a class=\"\" href=\"#Perceptrons\">Perceptrons</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Working-of-Perceptrons\">Working of Perceptrons</a></li>\n",
    "<li><a class=\"\" href=\"#Sigmoid-Neurons\">Sigmoid Neurons</a></li>\n",
    "</ol><li><a class=\"\" href=\"#Terminology\">Terminology</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Notation\">Notation</a></li>\n",
    "</ol><li><a class=\"\" href=\"#Designing-a-Neural-Network-to-Classify-Handwritten-Digits\">Designing a Neural Network to Classify Handwritten Digits</a></li>\n",
    "<ol><li><a class=\"\" href=\"#The-Architecture\">The Architecture</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptrons were developed in the 1950s and 1960s by the scientist Frank Rosenblatt, inspired by earlier work by Warren McCulloch and Walter Pitts. A perceptron takes several binary inputs, $x_1,x_2,\\ldots,$ and produces a single binary output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/0101.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working of Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example shown the perceptron has three inputs, $x_1,x_2,x_3$. In general it could have more or fewer inputs. Rosenblatt proposed a simple rule to compute the output. He introduced weights, $w_1,w_2,…,$ real numbers expressing the importance of the respective inputs to the output. The neuron's output, 0 or 1, is determined by whether the weighted sum $\\sum_j w_j x_j$ is less than or greater than some threshold value. Just like the weights, the threshold is a real number which is a parameter of the neuron. To put it in more precise algebraic terms:\n",
    "$$\n",
    "\\text{output}=\\begin{cases}\n",
    "\t\t\t0 & \\text{if }  \\sum_j w_j x_j \\le \\text{threshold} \\\\\n",
    "       1 & \\text{if }  \\sum_j w_j x_j >  \\text{threshold}\n",
    "\t\t \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above notation can be simplified by using a dot product instead of the sum and transfering the threshold to the left of the inequality. Using this notation, the equation for the output is:\n",
    "$$\n",
    "\\text{output}=\\begin{cases}\n",
    "            0 & \\text{if }  w \\cdot x -b\\le 0 \\\\\n",
    "       1 & \\text{if }  w \\cdot x -b>  0\n",
    "         \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b here is the bias of the perceptron. You can think of the bias as a measure of how easy it is to get the perceptron to output a 1. Or to put it in more biological terms, the bias is a measure of how easy it is to get the perceptron to fire. For a perceptron with a really big bias, it's extremely easy for the perceptron to output a 1. But if the bias is very negative, then it's difficult for the perceptron to output a 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like a perceptron, the sigmoid neuron has inputs, $x_1,x_2,\\ldots$. But instead of being just 0 or 1, these inputs can also take on any values between 0 and 1. So, for instance, 0.638… is a valid input for a sigmoid neuron. Also just like a perceptron, the sigmoid neuron has weights for each input, $w_1,w_2,\\ldots,$ and an overall bias, $b$. But the output is not 0 or 1. Instead, it's $\\sigma(w \\cdot x+b)$, where $\\sigma$ is called the sigmoid function.\n",
    "$$\n",
    "\\begin{align*} \n",
    "  \\sigma(z) \\equiv \\frac{1}{1+e^{-z}}\n",
    "\\end{align*}$$\n",
    "or using the inputs and weights,\n",
    "$$\n",
    "\\begin{align*} \n",
    "  \\sigma(z) \\equiv \\frac{1}{1+e^{-w \\cdot x-b}}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have the network:\n",
    "\n",
    "![](images/0102.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The leftmost layer in this network is called the *input layer*, and the neurons within the layer are called input neurons. The rightmost or *output layer* contains the output neurons, or, as in this case, a single output neuron. The middle layer is called a *hidden layer*. The network above has just a single hidden layer, but some networks have multiple hidden layers. For example, the following four-layer network has two hidden layers:\n",
    "\n",
    "![](images/0103.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Superscript $[l]$ denotes a quantity associated with the $l^{th}$ layer. \n",
    "    - Example: $a^{[L]}$ is the $L^{th}$ layer activation. $W^{[L]}$ and $b^{[L]}$ are the $L^{th}$ layer parameters.\n",
    "- Superscript $(i)$ denotes a quantity associated with the $i^{th}$ example. \n",
    "    - Example: $x^{(i)}$ is the $i^{th}$ training example.\n",
    "- Lowerscript $i$ denotes the $i^{th}$ entry of a vector.\n",
    "    - Example: $a^{[l]}_i$ denotes the $i^{th}$ entry of the $l^{th}$ layer's activations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Designing a Neural Network to Classify Handwritten Digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The design of the input and output layers in a network is often straightforward. For example, suppose we're trying to determine whether a handwritten image depicts a \"9\" or not. A natural way to design the network is to encode the intensities of the image pixels into the input neurons. If the image is a 64 by 64 greyscale image, then we'd have 4,096=64×64 input neurons, with the intensities scaled appropriately between 0 and 1. The output layer will contain just a single neuron, with output values of less than 0.5 indicating \"input image is not a 9\", and values greater than 0.5 indicating \"input image is a 9\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the design of the input and output layers of a neural network is often straightforward, there can be quite an art to the design of the hidden layers. In particular, it's not possible to sum up the design process for the hidden layers with a few simple rules of thumb. Instead, neural networks researchers have developed many design heuristics for the hidden layers, which help people get the behaviour they want out of their nets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recognize individual digits we will use a three-layer neural network:\n",
    "\n",
    "![](images/0104.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture we'll be using is a three layered network, as shown in the figure above.  Our training data for the network will consist of many 28 by 28 pixel images of scanned handwritten digits, and so the input layer contains 784=28×28 neurons. The input layer has been omitted for simplicity. The first hidden layer contains 15 neurons, and the second hidden layer contains 30 neurons. Since we want to classify the digits, the output layer contains 10 neurons, one for each digit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
