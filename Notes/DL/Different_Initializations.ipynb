{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Contents\">Contents<a href=\"#Contents\"></a></h2>\n",
    "        <ol>\n",
    "        <li><a class=\"\" href=\"#Imports\">Imports</a></li>\n",
    "<li><a class=\"\" href=\"#Initialization\">Initialization</a></li>\n",
    "<li><a class=\"\" href=\"#Different-Types-of-Initialization\">Different Types of Initialization</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Zero-Initialization\">Zero Initialization</a></li>\n",
    "<li><a class=\"\" href=\"#Random-Initialization\">Random Initialization</a></li>\n",
    "<li><a class=\"\" href=\"#He-Initialization\">He Initialization</a></li>\n",
    "<li><a class=\"\" href=\"#Glorot-Initialization\">Glorot Initialization</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training your neural network requires specifying an initial value of the weights. A well-chosen initialization method helps the learning process.\n",
    "\n",
    "A well-chosen initialization can:\n",
    "- Speed up the convergence of gradient descent\n",
    "- Increase the odds of gradient descent converging to a lower training (and generalization) error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Types of Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights of a neural network can be initialized in a variety of ways. Some of them are:\n",
    "1. Zero initialization: Initialize all weights to 0.\n",
    "2. Random initialization: Initialize all weights to random values.\n",
    "3. He initialization: Initialize all weights to random values drawn from a normal distribution with a mean of 0 and standard deviation of $\\sqrt{2 / n}$.\n",
    "4. Glorot initialization: Initialize all weights to random values drawn from a normal distribution with a mean of 0 and standard deviation of $\\sqrt{2 / (n + m)}$.\n",
    "5. Xavier initialization: Initialize all weights to random values drawn from a normal distribution with a mean of 0 and standard deviation of $\\sqrt{1 / n}$.\n",
    "Here $n$ is the number of input units, $m$ is the number of output units for the current layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/0301.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the most trivial initialization method. In this method, all\n",
    "- the weight matrices $(W^{[1]}, W^{[2]}, W^{[3]}, ..., W^{[L-1]}, W^{[L]})$\n",
    "  \n",
    " and\n",
    "- the bias vectors $(b^{[1]}, b^{[2]}, b^{[3]}, ..., b^{[L-1]}, b^{[L]})$\n",
    "  \n",
    "are initialized to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_zeros(layers_dims):\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(layers_dims)\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.zeros((layers_dims[l], layers_dims[l-1]))\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this type of initialization is of no use. Once the weights are initialized to zero, they remain zero throughout the training and the model doesn't learn anything. Let's see why this can happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the weights and biases are zero, multiplying by the weights creates the zero vector which gives 0 when the activation function is ReLU. As `z = 0`\n",
    "\n",
    "$$a = ReLU(z) = max(0, z) = 0$$\n",
    "\n",
    "At the classification layer, where the activation function is sigmoid you then get (for either input): \n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{ 1 + e^{-(z)}} = \\frac{1}{2} = y_{pred}$$\n",
    "\n",
    "As for every example you are getting a 0.5 chance of it being true our cost function becomes helpless in adjusting the weights.\n",
    "\n",
    "Your loss function:\n",
    "$$ \\mathcal{L}(a, y) =  - y  \\ln(y_{pred}) - (1-y)  \\ln(1-y_{pred})$$\n",
    "\n",
    "For `y=1`, `y_pred=0.5` it becomes:\n",
    "\n",
    "$$ \\mathcal{L}(0, 1) =  - (1)  \\ln(\\frac{1}{2}) = 0.6931471805599453$$\n",
    "\n",
    "For `y=0`, `y_pred=0.5` it becomes:\n",
    "\n",
    "$$ \\mathcal{L}(0, 0) =  - (1)  \\ln(\\frac{1}{2}) = 0.6931471805599453$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see with the prediction being 0.5 whether the actual (`y`) value is 1 or 0 we get the same loss value for both, so none of the weights get adjusted and we are stuck with the same old value of the weights. \n",
    "\n",
    "This is why we can see that the model is predicting 0 for every example!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, initializing all the weights to zero results in the network failing to break symmetry. This means that every neuron in each layer will learn the same thing, so we might as well be training a neural network with $n^{[l]}=1$ for every layer. This way, the network is no more powerful than a linear classifier like logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To break symmetry, initialize the weights randomly. Following random initialization, each neuron can then proceed to learn a different function of its inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_random(layers_dims):\n",
    "    parameters = {}\n",
    "    L = len(layers_dims) \n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1])*10\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With random initialization, the model starts with making prediction by random but starts to learn with every epoch.\n",
    "\n",
    "Random initialization can be used to break symmetry, however it is far from perfect. Poor initialization can lead to vanishing/exploding gradients, which also slows down the optimization algorithm. What's more initializing with overly large random numbers slows down the optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## He Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"He Initialization\"; this is named for the first author of He et al., 2015. This is the same as random intialization, the only different being that while the standard deviation for random initialization is1, the standard deviation for He initialization is $\\sqrt{\\frac{2}{\\text{dimension of the previous layer}}}$. Changing the standard deviation to $\\sqrt{\\frac{1}{\\text{dimension of the previous layer}}}$ gives the same **Xavier initialization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_he(layers_dims):\n",
    "    parameters = {}\n",
    "    L = len(layers_dims) - 1\n",
    "     \n",
    "    for l in range(1, L + 1):\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1])*(np.sqrt(2/(layers_dims[l-1])))\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glorot Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the standard deviation is $\\sqrt{\\frac{2}{\\text{dimension of the previous layer}+\\text{dimension of the current layer}}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_glorot(layers_dims):\n",
    "    parameters = {}\n",
    "    L = len(layers_dims) - 1\n",
    "     \n",
    "    for l in range(1, L + 1):\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1])*(np.sqrt(2/((layers_dims[l-1]+layers_dims[l]))))\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))        \n",
    "    return parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
