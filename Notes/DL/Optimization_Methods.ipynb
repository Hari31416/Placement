{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"Contents\">Contents<a href=\"#Contents\"></a></h2>\n",
    "        <ol>\n",
    "        <li><a class=\"\" href=\"#Imports\">Imports</a></li>\n",
    "<li><a class=\"\" href=\"#Optimization-Methods\">Optimization Methods</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Gradient-Descent\">Gradient Descent</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Stochastic-Gradient-Descent\">Stochastic Gradient Descent</a></li>\n",
    "<li><a class=\"\" href=\"#Mini-Batch-Gradient-Descent\">Mini-Batch Gradient Descent</a></li>\n",
    "</ol><li><a class=\"\" href=\"#Momentum\">Momentum</a></li>\n",
    "<li><a class=\"\" href=\"#RMSProp\">RMSProp</a></li>\n",
    "<li><a class=\"\" href=\"#Adam\">Adam</a></li>\n",
    "</ol><li><a class=\"\" href=\"#Learning-Rate-Decay-and-Scheduling\">Learning Rate Decay and Scheduling</a></li>\n",
    "<ol><li><a class=\"\" href=\"#Decay-on-every-iteration\">Decay on every iteration</a></li>\n",
    "<li><a class=\"\" href=\"#Fixed-Interval-Scheduling\">Fixed Interval Scheduling</a></li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a good optimization algorithm can be the difference between waiting days vs. just a few hours to get a good result. Until now, we've always used Gradient Descent to update the parameters and minimize the cost. Here, we'll talk about some other optimization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be including the gradient descent just for the sake of completeness. When you take gradient steps with respect to all  m  examples on each step, it is also called Batch Gradient Descent. The  gradient descent rule is, for $l = 1, ..., L$: \n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]}$$\n",
    "\n",
    "where L is the number of layers and $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_gd(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(1, L + 1):\n",
    "        parameters[\"W\" + str(l)] -= grads[\"dW\" + str(l)]*learning_rate\n",
    "        parameters[\"b\" + str(l)] -= grads[\"db\" + str(l)]*learning_rate\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent is a variant of Gradient Descent where you only update the parameters after seeing a single example.\n",
    "\n",
    "The update rule that you have just implemented does not change. What changes is that you would be computing gradients on just one training example at a time, rather than on the whole training set. The code examples below illustrate the difference between stochastic gradient descent and (batch) gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **(Batch) Gradient Descent**:\n",
    "\n",
    "``` python\n",
    "X = data_input\n",
    "Y = labels\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "for i in range(0, num_iterations):\n",
    "    # Forward propagation\n",
    "    a, caches = forward_propagation(X, parameters)\n",
    "    # Compute cost.\n",
    "    cost += compute_cost(a, Y)\n",
    "    # Backward propagation.\n",
    "    grads = backward_propagation(a, caches, parameters)\n",
    "    # Update parameters.\n",
    "    parameters = update_parameters(parameters, grads)\n",
    "        \n",
    "```\n",
    "\n",
    "- **Stochastic Gradient Descent**:\n",
    "\n",
    "```python\n",
    "X = data_input\n",
    "Y = labels\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "for i in range(0, num_iterations):\n",
    "    for j in range(0, m):\n",
    "        # Forward propagation\n",
    "        a, caches = forward_propagation(X[:,j], parameters)\n",
    "        # Compute cost\n",
    "        cost += compute_cost(a, Y[:,j])\n",
    "        # Backward propagation\n",
    "        grads = backward_propagation(a, caches, parameters)\n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">In Stochastic Gradient Descent, you use only 1 training example before updating the gradients. When the training set is large, SGD can be faster. But the parameters will \"oscillate\" toward the minimum rather than converge smoothly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/0401.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini-batch gradient descent uses an intermediate number of examples for each step. With mini-batch gradient descent, you loop over the mini-batches instead of looping over individual training examples. There are two steps involved in mini-batch gradient descent:\n",
    "- **Shuffle**: Create a shuffled version of the training set (X, Y) as shown below. Each column of X and Y represents a training example. Note that the random shuffling is done synchronously between X and Y. Such that after the shuffling the $i^{th}$ column of X is the example corresponding to the $i^{th}$ label in Y. The shuffling step ensures that examples will be split randomly into different mini-batches. \n",
    " \n",
    "![](images/0402.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Partition**: Partition the shuffled (X, Y) into mini-batches of size `mini_batch_size` (here 64). Note that the number of training examples is not always divisible by `mini_batch_size`. The last mini batch might be smaller, but you don't need to worry about this. When the final mini-batch is smaller than the full `mini_batch_size\n",
    "  \n",
    "![](images/0403.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    np.random.seed(seed)\n",
    "    m = X.shape[1]\n",
    "    mini_batches = []\n",
    "\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1, m))\n",
    "    \n",
    "    inc = mini_batch_size\n",
    "\n",
    "    num_complete_minibatches = np.floor(m / mini_batch_size) \n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, k*mini_batch_size:(k+1)*mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k*mini_batch_size:(k+1)*mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # For handling the end case (last mini-batch < mini_batch_size i.e less than 64)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, (k+1)*mini_batch_size:]\n",
    "        mini_batch_Y = shuffled_Y[:, (k+1)*mini_batch_size:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will \"oscillate\" toward convergence. Using momentum can reduce these oscillations.\n",
    "\n",
    "Momentum takes into account the past gradients to smooth out the update. The 'direction' of the previous gradients is stored in the variable  v . Formally, this will be the exponentially weighted average of the gradient on previous steps. You can also think of  v  as the \"velocity\" of a ball rolling downhill, building up speed (and momentum) according to the direction of the gradient/slope of the hill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this algorithm to work, we first intialize the *velocity* as a vector of zeros. After that the velocity as well as the weight parameters are updated using the formula:\n",
    "$$ \\begin{cases}\n",
    "v_{dW^{[l]}} = \\beta v_{dW^{[l]}} + (1 - \\beta) dW^{[l]} \\\\\n",
    "W^{[l]} = W^{[l]} - \\alpha v_{dW^{[l]}}\n",
    "\\end{cases}$$\n",
    "\n",
    "$$\\begin{cases}\n",
    "v_{db^{[l]}} = \\beta v_{db^{[l]}} + (1 - \\beta) db^{[l]} \\\\\n",
    "b^{[l]} = b^{[l]} - \\alpha v_{db^{[l]}} \n",
    "\\end{cases}$$\n",
    "for $l = 1, ..., L$\n",
    "where L is the number of layers and $\\beta$ is the momentum and $\\alpha$ is the learning rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n",
    "    L = len(parameters) // 2\n",
    "    for l in range(1, L + 1):\n",
    "        v[\"dW\" + str(l)] = beta*v[\"dW\" + str(l)] + (1 - beta)*grads[\"dW\" + str(l)]\n",
    "        v[\"db\" + str(l)] = beta*v[\"db\" + str(l)] + (1 - beta)*grads[\"db\" + str(l)]\n",
    "\n",
    "        parameters[\"W\" + str(l)] -= learning_rate*v[\"dW\" + str(l)]\n",
    "        parameters[\"b\" + str(l)] -= learning_rate*v[\"db\" + str(l)]\n",
    "    return parameters, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that**:\n",
    "- The velocity is initialized with zeros. So the algorithm will take a few iterations to \"build up\" velocity and start to take bigger steps.\n",
    "- If $\\beta = 0$, then this just becomes standard gradient descent without momentum. \n",
    "\n",
    "**How do you choose $\\beta$?**\n",
    "\n",
    "- The larger the momentum $\\beta$ is, the smoother the update, because it takes the past gradients into account more. But if $\\beta$ is too big, it could also smooth out the updates too much. \n",
    "- Common values for $\\beta$ range from 0.8 to 0.999. If you don't feel inclined to tune this, $\\beta = 0.9$ is often a reasonable default. \n",
    "- Tuning the optimal $\\beta$ for your model might require trying several values to see what works best in terms of reducing the value of the cost function $J$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent starts by quickly going down the steepest slope, then slowly goes down the bottom of the valley. It would be nice if the algorithm could detect this early on and correct its direction to point a bit more toward the global optimum. This is what RMSProp does. RMSProp keeps track of an exponentially decaying average of past squared gradients. It updates the parameter as:\n",
    "$$s ←\\beta s + (1-\\beta)∇_θJ(θ)⊗ ∇_θJ(θ)\\\\\n",
    "θ ←θ − η ∇_θJ(θ) ⊗\\sqrt{s +\\epsilon}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⊗ is the element-wise product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam is one of the most effective optimization algorithms for training neural networks. It combines ideas from RMSProp and Momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does Adam work?**\n",
    "1. It calculates an exponentially weighted average of past gradients, and stores it in variables $v$ (before bias correction) and $v^{corrected}$ (with bias correction). \n",
    "2. It calculates an exponentially weighted average of the squares of the past gradients, and  stores it in variables $s$ (before bias correction) and $s^{corrected}$ (with bias correction). \n",
    "3. It updates parameters in a direction based on combining information from \"1\" and \"2\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update rule is, for $l = 1, ..., L$: \n",
    "\n",
    "$$\\begin{cases}\n",
    "v_{dW^{[l]}} = \\beta_1 v_{dW^{[l]}} + (1 - \\beta_1) \\frac{\\partial \\mathcal{J} }{ \\partial W^{[l]} } \\\\\n",
    "v^{corrected}_{dW^{[l]}} = \\frac{v_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\\n",
    "s_{dW^{[l]}} = \\beta_2 s_{dW^{[l]}} + (1 - \\beta_2) (\\frac{\\partial \\mathcal{J} }{\\partial W^{[l]} })^2 \\\\\n",
    "s^{corrected}_{dW^{[l]}} = \\frac{s_{dW^{[l]}}}{1 - (\\beta_2)^t} \\\\\n",
    "W^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{dW^{[l]}}}{\\sqrt{s^{corrected}_{dW^{[l]}}} + \\varepsilon}\n",
    "\\end{cases}$$\n",
    "where:\n",
    "- t counts the number of steps taken of Adam \n",
    "- L is the number of layers\n",
    "- $\\beta_1$ and $\\beta_2$ are hyperparameters that control the two exponentially weighted averages. \n",
    "- $\\alpha$ is the learning rate\n",
    "- $\\varepsilon$ is a very small number to avoid dividing by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(\n",
    "    parameters, grads, v, s, t, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8\n",
    "):\n",
    "\n",
    "    L = len(parameters) // 2\n",
    "    v_corrected = {}\n",
    "    s_corrected = {}\n",
    "    for l in range(1, L + 1):\n",
    "        v[\"dW\" + str(l)] = beta1 * v[\"dW\" + str(l)] + (1 - beta1) * grads[\"dW\" + str(l)]\n",
    "        v[\"db\" + str(l)] = beta1 * v[\"db\" + str(l)] + (1 - beta1) * grads[\"db\" + str(l)]\n",
    "\n",
    "        v_corrected[\"dW\" + str(l)] = v[\"dW\" + str(l)] / (1 - (beta1) ** t)\n",
    "        v_corrected[\"db\" + str(l)] = v[\"db\" + str(l)] / (1 - (beta1) ** t)\n",
    "\n",
    "        s[\"dW\" + str(l)] = (\n",
    "            beta2 * s[\"dW\" + str(l)] + (1 - beta2) * (grads[\"dW\" + str(l)]) ** 2\n",
    "        )\n",
    "        s[\"db\" + str(l)] = (\n",
    "            beta2 * s[\"db\" + str(l)] + (1 - beta2) * (grads[\"db\" + str(l)]) ** 2\n",
    "        )\n",
    "\n",
    "        s_corrected[\"dW\" + str(l)] = s[\"dW\" + str(l)] / (1 - (beta2) ** t)\n",
    "        s_corrected[\"db\" + str(l)] = s[\"db\" + str(l)] / (1 - (beta2) ** t)\n",
    "\n",
    "        parameters[\"W\" + str(l)] -= (learning_rate * v_corrected[\"dW\" + str(l)]) / (\n",
    "            np.sqrt(s_corrected[\"dW\" + str(l)]) + epsilon\n",
    "        )\n",
    "        parameters[\"b\" + str(l)] -= (learning_rate * v_corrected[\"db\" + str(l)]) / (\n",
    "            np.sqrt(s_corrected[\"db\" + str(l)]) + epsilon\n",
    "        )\n",
    "\n",
    "    return parameters, v, s, v_corrected, s_corrected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Learning Rate Decay and Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the optimization algorithm, changing the learning rate can also speed up the training of the model. \n",
    "During the first part of training, our model can get away with taking large steps, but over time, using a fixed value for the learning rate alpha can cause our model to get stuck in a wide oscillation that never quite converges. But if we were to slowly reduce our learning rate alpha over time, we could then take smaller, slower steps that bring we closer to the minimum. This is the idea behind learning rate decay.\n",
    "\n",
    "Learning rate decay can be achieved by using either adaptive methods or pre-defined learning rate schedules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decay on every iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try one of the pre-defined schedules for learning rate decay, called exponential learning rate decay. It takes this mathematical form:\n",
    "$$\\alpha = \\frac{1}{1 + decayRate \\times epochNumber} \\alpha_{0}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method has the problem that as the epoch number increases, the learning rate becomes smaller and smaller and eventually becomes zero. This makes the model unable to learn anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed Interval Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to solve this problem is to use a fixed interval scheduling. This is a method that takes a fixed number of iterations and then reduces the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mathematical form of this schedule is:\n",
    "$$\\alpha = \\frac{1}{1 + decayRate \\times \\lfloor\\frac{epochNum}{timeInterval}\\rfloor} \\alpha_{0}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
